# -*- coding: utf-8 -*-
"""
é¡¹ç›®4ï¼šæƒ…æ„Ÿåˆ†æ
================
è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†é¡¹ç›®ï¼Œåˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ã€‚

é¡¹ç›®ç›®æ ‡ï¼š
- ç†è§£æ–‡æœ¬æ•°æ®é¢„å¤„ç†
- å­¦ä¹ æ–‡æœ¬ç‰¹å¾æå–
- è®­ç»ƒæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
- åˆ†ææ–°æ–‡æœ¬æƒ…æ„Ÿ

è¿è¡Œæ–¹å¼ï¼š
    python sentiment_analysis.py
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import re

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºï¼ˆå…¼å®¹ä¸åŒç³»ç»Ÿï¼‰
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
elif platform.system() == 'Darwin':  # Mac
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang SC']
else:  # Linux
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def load_data():
    """
    åŠ è½½æƒ…æ„Ÿåˆ†ææ•°æ®

    è¿”å›:
        tuple: (æ–‡æœ¬åˆ—è¡¨, æ ‡ç­¾åˆ—è¡¨)

    æ•°æ®è¯´æ˜:
        ä½¿ç”¨æ¨¡æ‹Ÿçš„ä¸­æ–‡è¯„è®ºæ•°æ®
        æ ‡ç­¾ï¼š1=æ­£é¢ï¼Œ0=è´Ÿé¢
    """
    print("=" * 50)
    print("ğŸ˜Š æƒ…æ„Ÿåˆ†æé¡¹ç›®")
    print("=" * 50)
    print()

    print("ã€æ­¥éª¤1ï¼šåŠ è½½æ•°æ®ã€‘")

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ä¸­æ–‡è¯„è®ºæ•°æ®
    positive_comments = [
        "è¿™ä¸ªäº§å“éå¸¸å¥½ï¼Œæˆ‘å¾ˆå–œæ¬¢",
        "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„",
        "è´¨é‡ä¼˜ç§€ï¼Œå€¼å¾—æ¨è",
        "ç‰©ç¾ä»·å»‰ï¼Œè¶…çº§æ£’",
        "éå¸¸å¼€å¿ƒï¼Œä¸‹æ¬¡è¿˜ä¼šæ¥",
        "ä½“éªŒéå¸¸å¥½ï¼Œäº”æ˜Ÿå¥½è¯„",
        "å¤ªæ£’äº†ï¼Œè¶…å‡ºé¢„æœŸ",
        "å®Œç¾ï¼Œæ²¡æœ‰ä»»ä½•é—®é¢˜",
        "å®¢æœå¾ˆæœ‰è€å¿ƒï¼Œå¥½è¯„",
        "ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…å¾ˆå¥½",
        "é¢œè‰²å¾ˆå¥½çœ‹ï¼Œå¾ˆå–œæ¬¢",
        "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œæ¨èè´­ä¹°",
        "ç”¨èµ·æ¥å¾ˆèˆ’æœï¼Œè´¨é‡å¥½",
        "å‘³é“å¾ˆå¥½ï¼Œå¾ˆæ–°é²œ",
        "æ•ˆæœæ˜æ˜¾ï¼Œéå¸¸æ»¡æ„",
        "è¿™ä¸ªçœŸçš„å¾ˆä¸é”™",
        "æ¯”æƒ³è±¡ä¸­æ›´å¥½",
        "éå¸¸æ»¡æ„è¿™æ¬¡è´­ç‰©",
        "å€¼å¾—è´­ä¹°ï¼Œæ¨èç»™å¤§å®¶",
        "å¤ªå–œæ¬¢äº†ï¼Œçˆ±ä¸é‡Šæ‰‹",
    ]

    negative_comments = [
        "è´¨é‡å¤ªå·®äº†ï¼Œå¾ˆå¤±æœ›",
        "æœåŠ¡æ€åº¦æ¶åŠ£ï¼Œä¸æ»¡æ„",
        "å®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼",
        "å¾ˆç³Ÿç³•çš„ä¸€æ¬¡ä½“éªŒ",
        "é€€è´§äº†ï¼Œéå¸¸ä¸æ»¡æ„",
        "å·®è¯„ï¼Œä¸ä¼šå†æ¥äº†",
        "è´¨é‡æœ‰é—®é¢˜ï¼Œä¸æ¨è",
        "æ”¶åˆ°çš„å•†å“æœ‰æŸå",
        "å®¢æœæ€åº¦å¾ˆå·®",
        "ç‰©æµå¤ªæ…¢äº†ï¼Œå·®è¯„",
        "é¢œè‰²å’Œå›¾ç‰‡ä¸ç¬¦",
        "æ€§ä»·æ¯”å¾ˆä½ï¼Œä¸åˆ’ç®—",
        "ç”¨èµ·æ¥ä¸èˆ’æœï¼Œè´¨é‡å·®",
        "å‘³é“ä¸å¥½ï¼Œä¸æ–°é²œ",
        "æ²¡æœ‰æ•ˆæœï¼Œå¾ˆå¤±æœ›",
        "è¿™ä¸ªçœŸçš„å¾ˆå·®åŠ²",
        "æ¯”æƒ³è±¡ä¸­å·®å¾ˆå¤š",
        "éå¸¸ä¸æ»¡æ„è¿™æ¬¡è´­ç‰©",
        "ä¸å€¼å¾—è´­ä¹°",
        "å¾ˆå¤±æœ›ï¼Œä¸æ¨è",
    ]

    # åˆå¹¶æ•°æ®
    texts = positive_comments + negative_comments
    labels = [1] * len(positive_comments) + [0] * len(negative_comments)

    # æ‰“ä¹±æ•°æ®
    np.random.seed(42)
    indices = np.random.permutation(len(texts))
    texts = [texts[i] for i in indices]
    labels = [labels[i] for i in indices]

    print(f"  æ ·æœ¬æ•°é‡ï¼š{len(texts)}")
    print(f"  æ­£é¢è¯„è®ºï¼š{sum(labels)}")
    print(f"  è´Ÿé¢è¯„è®ºï¼š{len(labels) - sum(labels)}")
    print()

    return texts, labels


def explore_data(texts, labels):
    """
    æ¢ç´¢æ–‡æœ¬æ•°æ®

    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        labels: æ ‡ç­¾åˆ—è¡¨

    åŠŸèƒ½:
        - æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
        - ç»Ÿè®¡æ–‡æœ¬é•¿åº¦
        - åˆ†æè¯é¢‘
    """
    print("ã€æ­¥éª¤2ï¼šæ¢ç´¢æ•°æ®ã€‘")

    # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®
    print("  æ•°æ®ç¤ºä¾‹ï¼š")
    print("  ---------- æ­£é¢è¯„è®º ----------")
    for text in texts[:3]:
        if labels[texts.index(text)] == 1:
            print(f"    {text}")

    print("  ---------- è´Ÿé¢è¯„è®º ----------")
    for i in range(len(texts)):
        if labels[i] == 0:
            print(f"    {texts[i]}")
            if i >= 22:  # åªæ˜¾ç¤º3æ¡
                break
    print()

    # ç»Ÿè®¡æ–‡æœ¬é•¿åº¦
    lengths = [len(text) for text in texts]
    print(f"  æ–‡æœ¬é•¿åº¦ç»Ÿè®¡ï¼š")
    print(f"    æœ€çŸ­ï¼š{min(lengths)} å­—ç¬¦")
    print(f"    æœ€é•¿ï¼š{max(lengths)} å­—ç¬¦")
    print(f"    å¹³å‡ï¼š{np.mean(lengths):.1f} å­—ç¬¦")
    print()

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
    axes[0].hist([l for l, y in zip(lengths, labels) if y == 1],
                 alpha=0.7, label='æ­£é¢', bins=10)
    axes[0].hist([l for l, y in zip(lengths, labels) if y == 0],
                 alpha=0.7, label='è´Ÿé¢', bins=10)
    axes[0].set_xlabel('æ–‡æœ¬é•¿åº¦')
    axes[0].set_ylabel('æ•°é‡')
    axes[0].set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')
    axes[0].legend()

    # ç±»åˆ«åˆ†å¸ƒ
    counts = [sum(labels), len(labels) - sum(labels)]
    axes[1].bar(['æ­£é¢', 'è´Ÿé¢'], counts, color=['green', 'red'])
    axes[1].set_ylabel('æ•°é‡')
    axes[1].set_title('ç±»åˆ«åˆ†å¸ƒ')

    plt.tight_layout()

    output_dir = '/mnt/c/dev/python/qqstudy/ai_intro'
    plt.savefig(os.path.join(output_dir, 'sentiment_data.png'), dpi=100)
    print("  âœ… æ•°æ®å¯è§†åŒ–å·²ä¿å­˜åˆ°ï¼šsentiment_data.png")
    plt.close()
    print()


def preprocess_text(text):
    """
    é¢„å¤„ç†æ–‡æœ¬

    å‚æ•°:
        text: åŸå§‹æ–‡æœ¬

    è¿”å›:
        å¤„ç†åçš„æ–‡æœ¬

    åŠŸèƒ½:
        - å»é™¤æ ‡ç‚¹ç¬¦å·
        - å»é™¤å¤šä½™ç©ºæ ¼
    """
    # å»é™¤æ ‡ç‚¹
    text = re.sub(r'[^\w\s]', '', text)
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def extract_features(texts, method='count'):
    """
    æå–æ–‡æœ¬ç‰¹å¾

    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        method: ç‰¹å¾æå–æ–¹æ³•ï¼ˆ'count' æˆ– 'tfidf'ï¼‰

    è¿”å›:
        tuple: (ç‰¹å¾çŸ©é˜µ, å‘é‡åŒ–å™¨)

    ç‰¹å¾æå–è¯´æ˜:
        - Count: è¯é¢‘ç»Ÿè®¡ï¼ˆæ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°ï¼‰
        - TF-IDF: è€ƒè™‘è¯çš„é‡è¦æ€§
    """
    print("ã€æ­¥éª¤3ï¼šç‰¹å¾æå–ã€‘")

    # é¢„å¤„ç†
    processed_texts = [preprocess_text(text) for text in texts]

    # é€‰æ‹©å‘é‡åŒ–æ–¹æ³•
    if method == 'count':
        print("  ä½¿ç”¨æ–¹æ³•ï¼šè¯é¢‘ç»Ÿè®¡ (Count Vectorizer)")
        vectorizer = CountVectorizer()
    else:
        print("  ä½¿ç”¨æ–¹æ³•ï¼šTF-IDF")
        vectorizer = TfidfVectorizer()

    # è½¬æ¢æ–‡æœ¬ä¸ºç‰¹å¾çŸ©é˜µ
    X = vectorizer.fit_transform(processed_texts)

    print(f"  ç‰¹å¾æ•°é‡ï¼š{X.shape[1]}")
    print(f"  ç‰¹å¾çŸ©é˜µå½¢çŠ¶ï¼š{X.shape}")
    print()

    return X, vectorizer


def train_model(X, labels, test_size=0.3, model_type='nb'):
    """
    è®­ç»ƒæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹

    å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        labels: æ ‡ç­¾åˆ—è¡¨
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        model_type: æ¨¡å‹ç±»å‹ï¼ˆ'nb'=æœ´ç´ è´å¶æ–¯, 'lr'=é€»è¾‘å›å½’ï¼‰

    è¿”å›:
        tuple: (æ¨¡å‹, X_train, X_test, y_train, y_test)
    """
    print("ã€æ­¥éª¤4ï¼šè®­ç»ƒæ¨¡å‹ã€‘")

    # è½¬æ¢æ ‡ç­¾ä¸ºæ•°ç»„
    y = np.array(labels)

    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

    print(f"  è®­ç»ƒé›†å¤§å°ï¼š{X_train.shape[0]}")
    print(f"  æµ‹è¯•é›†å¤§å°ï¼š{X_test.shape[0]}")
    print()

    # é€‰æ‹©æ¨¡å‹
    if model_type == 'nb':
        print("  ä½¿ç”¨ç®—æ³•ï¼šæœ´ç´ è´å¶æ–¯")
        model = MultinomialNB()
    else:
        print("  ä½¿ç”¨ç®—æ³•ï¼šé€»è¾‘å›å½’")
        model = LogisticRegression(max_iter=1000)

    # è®­ç»ƒ
    model.fit(X_train, y_train)

    print("  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print()

    return model, X_train, X_test, y_train, y_test


def evaluate_model(model, X_test, y_test):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X_test: æµ‹è¯•ç‰¹å¾
        y_test: æµ‹è¯•æ ‡ç­¾
    """
    print("ã€æ­¥éª¤5ï¼šè¯„ä¼°æ¨¡å‹ã€‘")

    # é¢„æµ‹
    y_pred = model.predict(X_test)

    # å‡†ç¡®ç‡
    accuracy = accuracy_score(y_test, y_pred)
    print(f"  å‡†ç¡®ç‡ï¼š{accuracy:.2%}")
    print()

    # åˆ†ç±»æŠ¥å‘Š
    print("  åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred, target_names=['è´Ÿé¢', 'æ­£é¢']))
    print()

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(6, 5))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.colorbar()
    tick_marks = ['è´Ÿé¢', 'æ­£é¢']
    plt.xticks([0, 1], tick_marks)
    plt.yticks([0, 1], tick_marks)

    # åœ¨æ ¼å­ä¸­æ˜¾ç¤ºæ•°å­—
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()

    output_dir = '/mnt/c/dev/python/qqstudy/ai_intro'
    plt.savefig(os.path.join(output_dir, 'sentiment_confusion_matrix.png'), dpi=100)
    print("  âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°ï¼šsentiment_confusion_matrix.png")
    plt.close()
    print()


def predict_new(model, vectorizer, new_texts):
    """
    é¢„æµ‹æ–°æ–‡æœ¬çš„æƒ…æ„Ÿ

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        vectorizer: å‘é‡åŒ–å™¨
        new_texts: æ–°æ–‡æœ¬åˆ—è¡¨

    è¿”å›:
        é¢„æµ‹ç»“æœåˆ—è¡¨
    """
    print("ã€æ­¥éª¤6ï¼šé¢„æµ‹æ–°æ–‡æœ¬ã€‘")

    # é¢„å¤„ç†
    processed = [preprocess_text(text) for text in new_texts]

    # å‘é‡åŒ–
    X_new = vectorizer.transform(processed)

    # é¢„æµ‹
    predictions = model.predict(X_new)
    probabilities = model.predict_proba(X_new)

    print("  é¢„æµ‹ç»“æœï¼š")
    for text, pred, prob in zip(new_texts, predictions, probabilities):
        sentiment = "æ­£é¢ ğŸ˜Š" if pred == 1 else "è´Ÿé¢ ğŸ˜"
        confidence = prob[pred]
        print(f"    '{text}'")
        print(f"      â†’ {sentiment} (ç½®ä¿¡åº¦: {confidence:.2%})")
        print()

    return predictions


def analyze_keywords(vectorizer, model, top_n=10):
    """
    åˆ†æå…³é”®è¯çš„é‡è¦æ€§

    å‚æ•°:
        vectorizer: å‘é‡åŒ–å™¨
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        top_n: æ˜¾ç¤ºçš„å…³é”®è¯æ•°é‡
    """
    print("ã€æ­¥éª¤7ï¼šå…³é”®è¯åˆ†æã€‘")

    # è·å–ç‰¹å¾è¯
    feature_names = vectorizer.get_feature_names_out()

    # è·å–æ¨¡å‹ç³»æ•°ï¼ˆé€»è¾‘å›å½’ï¼‰
    if hasattr(model, 'coef_'):
        coef = model.coef_[0]

        # æ­£é¢è¯ï¼ˆç³»æ•°æœ€å¤§ï¼‰
        positive_indices = np.argsort(coef)[-top_n:][::-1]
        print(f"  æœ€èƒ½ä»£è¡¨æ­£é¢æƒ…æ„Ÿçš„è¯ï¼š")
        for idx in positive_indices:
            print(f"    {feature_names[idx]}: {coef[idx]:.3f}")

        print()

        # è´Ÿé¢è¯ï¼ˆç³»æ•°æœ€å°ï¼‰
        negative_indices = np.argsort(coef)[:top_n]
        print(f"  æœ€èƒ½ä»£è¡¨è´Ÿé¢æƒ…æ„Ÿçš„è¯ï¼š")
        for idx in negative_indices:
            print(f"    {feature_names[idx]}: {coef[idx]:.3f}")
    else:
        print("  æ³¨ï¼šæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„å…³é”®è¯åˆ†æè¾ƒå¤æ‚ï¼Œå»ºè®®ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹")

    print()


def simple_sentiment_dict():
    """
    åŸºäºæƒ…æ„Ÿè¯å…¸çš„ç®€å•æƒ…æ„Ÿåˆ†æ

    å±•ç¤ºæœ€åŸºç¡€çš„æƒ…æ„Ÿåˆ†ææ–¹æ³•
    """
    print("ã€é™„åŠ ï¼šæƒ…æ„Ÿè¯å…¸æ–¹æ³•ã€‘")

    # å®šä¹‰æƒ…æ„Ÿè¯å…¸
    positive_words = ['å¥½', 'æ£’', 'å–œæ¬¢', 'å¼€å¿ƒ', 'æ»¡æ„', 'ä¼˜ç§€', 'æ¨è', 'å®Œç¾', 'å¥½è¯„', 'å–œæ¬¢']
    negative_words = ['å·®', 'çƒ‚', 'è®¨åŒ', 'éš¾è¿‡', 'å¤±æœ›', 'ç³Ÿç³•', 'å·®è¯„', 'é—®é¢˜', 'æŸå', 'ä¸æ»¡']

    def analyze(text):
        """
        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›:
            æƒ…æ„Ÿç±»åˆ«å’Œå¾—åˆ†
        """
        pos_count = sum(1 for w in positive_words if w in text)
        neg_count = sum(1 for w in negative_words if w in text)

        score = pos_count - neg_count
        if score > 0:
            return "æ­£é¢", score
        elif score < 0:
            return "è´Ÿé¢", score
        else:
            return "ä¸­æ€§", score

    print("  åŸºäºæƒ…æ„Ÿè¯å…¸çš„ç®€å•åˆ†æï¼š")

    test_texts = [
        "è¿™ä¸ªäº§å“éå¸¸å¥½ï¼Œæˆ‘å¾ˆæ»¡æ„",
        "è´¨é‡å¤ªå·®äº†ï¼Œå¾ˆå¤±æœ›",
        "æ™®é€šçš„äº§å“ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„"
    ]

    for text in test_texts:
        sentiment, score = analyze(text)
        print(f"    '{text}' â†’ {sentiment} (å¾—åˆ†: {score})")

    print()
    print("  æ³¨ï¼šæƒ…æ„Ÿè¯å…¸æ–¹æ³•ç®€å•ä½†ä¸å¤Ÿç²¾ç¡®ï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•æ›´å‡†ç¡®")
    print()


def main():
    """
    ä¸»ç¨‹åºå…¥å£

    æ‰§è¡Œå®Œæ•´çš„æƒ…æ„Ÿåˆ†ææµç¨‹ï¼š
    1. åŠ è½½æ•°æ®
    2. æ¢ç´¢æ•°æ®
    3. ç‰¹å¾æå–
    4. è®­ç»ƒæ¨¡å‹
    5. è¯„ä¼°æ¨¡å‹
    6. é¢„æµ‹æ–°æ–‡æœ¬
    7. åˆ†æå…³é”®è¯
    """
    # 1. åŠ è½½æ•°æ®
    texts, labels = load_data()

    # 2. æ¢ç´¢æ•°æ®
    explore_data(texts, labels)

    # 3. ç‰¹å¾æå–
    X, vectorizer = extract_features(texts, method='count')

    # 4. è®­ç»ƒæ¨¡å‹
    model, X_train, X_test, y_train, y_test = train_model(X, labels, model_type='nb')

    # 5. è¯„ä¼°æ¨¡å‹
    evaluate_model(model, X_test, y_test)

    # 6. é¢„æµ‹æ–°æ–‡æœ¬
    new_texts = [
        "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œéå¸¸æ¨è",
        "è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼å¾—ä¸€ä¹°",
        "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œç‰©æµä¹Ÿå¾ˆå¿«"
    ]
    predict_new(model, vectorizer, new_texts)

    # 7. åˆ†æå…³é”®è¯ï¼ˆä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹ï¼‰
    print("  ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹åˆ†æå…³é”®è¯...")
    X_lr, vectorizer_lr = extract_features(texts, method='tfidf')
    model_lr, _, _, _, _ = train_model(X_lr, labels, model_type='lr')
    analyze_keywords(vectorizer_lr, model_lr)

    # 8. å±•ç¤ºæƒ…æ„Ÿè¯å…¸æ–¹æ³•
    simple_sentiment_dict()

    print("=" * 50)
    print("ğŸ‰ é¡¹ç›®å®Œæˆï¼")
    print("=" * 50)
    print()
    print("ä½ å­¦ä¼šäº†ï¼š")
    print("  âœ“ æ–‡æœ¬æ•°æ®é¢„å¤„ç†")
    print("  âœ“ æ–‡æœ¬ç‰¹å¾æå–ï¼ˆCount, TF-IDFï¼‰")
    print("  âœ“ è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹")
    print("  âœ“ æƒ…æ„Ÿåˆ†æåŸºæœ¬æ–¹æ³•")
    print("  âœ“ å…³é”®è¯é‡è¦æ€§åˆ†æ")
    print()


if __name__ == "__main__":
    main()
