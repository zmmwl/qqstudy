# -*- coding: utf-8 -*-
"""
é¡¹ç›®1ï¼šé¸¢å°¾èŠ±åˆ†ç±»å™¨
====================
è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œä½¿ç”¨KNNç®—æ³•å¯¹é¸¢å°¾èŠ±è¿›è¡Œåˆ†ç±»ã€‚

é¡¹ç›®ç›®æ ‡ï¼š
- åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
- è®­ç»ƒKNNåˆ†ç±»æ¨¡å‹
- è¯„ä¼°æ¨¡å‹æ€§èƒ½
- å¯è§†åŒ–ç»“æœ
- ä¿å­˜æ¨¡å‹ä¾›åç»­ä½¿ç”¨

è¿è¡Œæ–¹å¼ï¼š
    python iris_classifier.py
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import os

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºï¼ˆå…¼å®¹ä¸åŒç³»ç»Ÿï¼‰
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
elif platform.system() == 'Darwin':  # Mac
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang SC']
else:  # Linux
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def load_data():
    """
    åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†

    è¿”å›:
        tuple: (ç‰¹å¾æ•°æ®X, æ ‡ç­¾æ•°æ®y, ç‰¹å¾åç§°, ç±»åˆ«åç§°)

    æ•°æ®è¯´æ˜:
        - 150ä¸ªæ ·æœ¬
        - 4ä¸ªç‰¹å¾ï¼šèŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ã€èŠ±ç“£é•¿åº¦ã€èŠ±ç“£å®½åº¦
        - 3ä¸ªç±»åˆ«ï¼šå±±é¸¢å°¾ã€å˜è‰²é¸¢å°¾ã€ç»´å‰å°¼äºšé¸¢å°¾
    """
    print("=" * 50)
    print("ğŸŒº é¸¢å°¾èŠ±åˆ†ç±»å™¨")
    print("=" * 50)
    print()

    print("ã€æ­¥éª¤1ï¼šåŠ è½½æ•°æ®ã€‘")

    # ä»sklearnåŠ è½½å†…ç½®æ•°æ®é›†
    iris = datasets.load_iris()

    X = iris.data          # ç‰¹å¾æ•°æ®
    y = iris.target        # æ ‡ç­¾
    feature_names = iris.feature_names    # ç‰¹å¾åç§°
    target_names = iris.target_names      # ç±»åˆ«åç§°

    print(f"  æ ·æœ¬æ•°é‡ï¼š{len(X)}")
    print(f"  ç‰¹å¾æ•°é‡ï¼š{len(feature_names)}")
    print(f"  ç‰¹å¾åç§°ï¼š{feature_names}")
    print(f"  ç±»åˆ«æ•°é‡ï¼š{len(target_names)}")
    print(f"  ç±»åˆ«åç§°ï¼š{target_names}")
    print()

    return X, y, feature_names, target_names


def explore_data(X, y, feature_names, target_names):
    """
    æ¢ç´¢å’Œå¯è§†åŒ–æ•°æ®

    å‚æ•°:
        X: ç‰¹å¾æ•°æ®ï¼ˆnumpyæ•°ç»„ï¼‰
        y: æ ‡ç­¾æ•°æ®ï¼ˆnumpyæ•°ç»„ï¼‰
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        target_names: ç±»åˆ«åç§°åˆ—è¡¨

    åŠŸèƒ½:
        - æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        - ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    """
    print("ã€æ­¥éª¤2ï¼šæ¢ç´¢æ•°æ®ã€‘")

    # è½¬æ¢ä¸ºDataFrameæ–¹ä¾¿åˆ†æ
    df = pd.DataFrame(X, columns=feature_names)
    df['species'] = [target_names[i] for i in y]

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("  æ•°æ®ç»Ÿè®¡ï¼š")
    print(df.describe().round(2).to_string())
    print()

    # å¯è§†åŒ–
    print("  æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # å›¾1ï¼šèŠ±ç“£é•¿åº¦ vs èŠ±ç“£å®½åº¦
    colors = ['red', 'green', 'blue']
    for i, name in enumerate(target_names):
        mask = df['species'] == name
        axes[0, 0].scatter(
            df.loc[mask, 'petal length (cm)'],
            df.loc[mask, 'petal width (cm)'],
            c=colors[i], label=name, alpha=0.7, s=50
        )
    axes[0, 0].set_xlabel('èŠ±ç“£é•¿åº¦ (cm)')
    axes[0, 0].set_ylabel('èŠ±ç“£å®½åº¦ (cm)')
    axes[0, 0].set_title('èŠ±ç“£ç‰¹å¾åˆ†å¸ƒ')
    axes[0, 0].legend()

    # å›¾2ï¼šèŠ±è¼é•¿åº¦ vs èŠ±è¼å®½åº¦
    for i, name in enumerate(target_names):
        mask = df['species'] == name
        axes[0, 1].scatter(
            df.loc[mask, 'sepal length (cm)'],
            df.loc[mask, 'sepal width (cm)'],
            c=colors[i], label=name, alpha=0.7, s=50
        )
    axes[0, 1].set_xlabel('èŠ±è¼é•¿åº¦ (cm)')
    axes[0, 1].set_ylabel('èŠ±è¼å®½åº¦ (cm)')
    axes[0, 1].set_title('èŠ±è¼ç‰¹å¾åˆ†å¸ƒ')
    axes[0, 1].legend()

    # å›¾3ï¼šå„ç±»åˆ«èŠ±ç“£é•¿åº¦ç®±çº¿å›¾
    data_by_species = [df.loc[df['species'] == name, 'petal length (cm)'].values
                       for name in target_names]
    axes[1, 0].boxplot(data_by_species, labels=target_names)
    axes[1, 0].set_ylabel('èŠ±ç“£é•¿åº¦ (cm)')
    axes[1, 0].set_title('å„ç±»åˆ«èŠ±ç“£é•¿åº¦åˆ†å¸ƒ')

    # å›¾4ï¼šå„ç±»åˆ«æ•°é‡
    df['species'].value_counts().plot(kind='bar', ax=axes[1, 1],
                                       color=['red', 'green', 'blue'])
    axes[1, 1].set_xlabel('å“ç§')
    axes[1, 1].set_ylabel('æ•°é‡')
    axes[1, 1].set_title('å„ç±»åˆ«æ ·æœ¬æ•°é‡')
    axes[1, 1].tick_params(axis='x', rotation=0)

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    output_dir = '/mnt/c/dev/python/qqstudy/ai_intro'
    plt.savefig(os.path.join(output_dir, 'iris_analysis.png'), dpi=100)
    print(f"  âœ… å›¾è¡¨å·²ä¿å­˜åˆ°ï¼širis_analysis.png")
    plt.close()
    print()


def train_model(X, y, test_size=0.3, random_state=42, n_neighbors=5):
    """
    è®­ç»ƒKNNåˆ†ç±»æ¨¡å‹

    å‚æ•°:
        X: ç‰¹å¾æ•°æ®ï¼ˆnumpyæ•°ç»„ï¼‰
        y: æ ‡ç­¾æ•°æ®ï¼ˆnumpyæ•°ç»„ï¼‰
        test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.3ï¼‰
        random_state: éšæœºç§å­ï¼ˆé»˜è®¤42ï¼‰
        n_neighbors: KNNçš„Kå€¼ï¼ˆé»˜è®¤5ï¼‰

    è¿”å›:
        tuple: (æ¨¡å‹, X_train, X_test, y_train, y_test)
    """
    print("ã€æ­¥éª¤3ï¼šè®­ç»ƒæ¨¡å‹ã€‘")

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    print(f"  è®­ç»ƒé›†å¤§å°ï¼š{len(X_train)}")
    print(f"  æµ‹è¯•é›†å¤§å°ï¼š{len(X_test)}")
    print(f"  ä½¿ç”¨ç®—æ³•ï¼šKNN (K={n_neighbors})")
    print()

    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
    model.fit(X_train, y_train)

    print("  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print()

    return model, X_train, X_test, y_train, y_test


def evaluate_model(model, X_test, y_test, target_names):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X_test: æµ‹è¯•ç‰¹å¾
        y_test: æµ‹è¯•æ ‡ç­¾
        target_names: ç±»åˆ«åç§°

    åŠŸèƒ½:
        - è®¡ç®—å‡†ç¡®ç‡
        - æ˜¾ç¤ºåˆ†ç±»æŠ¥å‘Š
        - ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    """
    print("ã€æ­¥éª¤4ï¼šè¯„ä¼°æ¨¡å‹ã€‘")

    # é¢„æµ‹
    y_pred = model.predict(X_test)

    # å‡†ç¡®ç‡
    accuracy = accuracy_score(y_test, y_pred)
    print(f"  å‡†ç¡®ç‡ï¼š{accuracy:.2%}")
    print()

    # åˆ†ç±»æŠ¥å‘Š
    print("  åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred, target_names=target_names))

    # æ··æ·†çŸ©é˜µå¯è§†åŒ–
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.colorbar()
    tick_marks = np.arange(len(target_names))
    plt.xticks(tick_marks, target_names)
    plt.yticks(tick_marks, target_names)

    # åœ¨æ ¼å­ä¸­æ˜¾ç¤ºæ•°å­—
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()

    output_dir = '/mnt/c/dev/python/qqstudy/ai_intro'
    plt.savefig(os.path.join(output_dir, 'iris_confusion_matrix.png'), dpi=100)
    print("  âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°ï¼širis_confusion_matrix.png")
    plt.close()
    print()


def predict_new(model, target_names):
    """
    ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ–°æ•°æ®

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        target_names: ç±»åˆ«åç§°

    åŠŸèƒ½:
        - å¯¹å‡ ä¸ªæ–°æ ·æœ¬è¿›è¡Œé¢„æµ‹
        - æ˜¾ç¤ºé¢„æµ‹ç»“æœå’Œæ¦‚ç‡
    """
    print("ã€æ­¥éª¤5ï¼šé¢„æµ‹æ–°æ•°æ®ã€‘")

    # å®šä¹‰ä¸€äº›æ–°çš„é¸¢å°¾èŠ±æ ·æœ¬
    # [èŠ±è¼é•¿åº¦, èŠ±è¼å®½åº¦, èŠ±ç“£é•¿åº¦, èŠ±ç“£å®½åº¦]
    new_samples = np.array([
        [5.1, 3.5, 1.4, 0.2],  # å¯èƒ½æ˜¯å±±é¸¢å°¾
        [6.7, 3.0, 5.2, 2.3],  # å¯èƒ½æ˜¯ç»´å‰å°¼äºšé¸¢å°¾
        [5.9, 3.0, 4.2, 1.5],  # å¯èƒ½æ˜¯å˜è‰²é¸¢å°¾
    ])

    print("  æ–°æ ·æœ¬é¢„æµ‹ï¼š")
    print("  èŠ±è¼é•¿åº¦  èŠ±è¼å®½åº¦  èŠ±ç“£é•¿åº¦  èŠ±ç“£å®½åº¦  â†’  é¢„æµ‹å“ç§")
    print("  --------  --------  --------  --------     --------")

    predictions = model.predict(new_samples)
    probabilities = model.predict_proba(new_samples)

    for i, (sample, pred, prob) in enumerate(zip(new_samples, predictions, probabilities)):
        print(f"    {sample[0]:.1f}      {sample[1]:.1f}      {sample[2]:.1f}      {sample[3]:.1f}    â†’  {target_names[pred]}")
        print(f"                                              æ¦‚ç‡ï¼š{prob}")
        print()

    print()


def find_best_k(X, y, k_range=range(1, 21)):
    """
    å¯»æ‰¾æœ€ä½³çš„Kå€¼

    å‚æ•°:
        X: ç‰¹å¾æ•°æ®
        y: æ ‡ç­¾æ•°æ®
        k_range: Kå€¼èŒƒå›´

    åŠŸèƒ½:
        - æµ‹è¯•ä¸åŒKå€¼
        - ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
        - æ‰¾å‡ºæœ€ä½³Kå€¼
    """
    print("ã€æ­¥éª¤6ï¼šå¯»æ‰¾æœ€ä½³Kå€¼ã€‘")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    k_scores = []

    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, y_train)
        score = knn.score(X_test, y_test)
        k_scores.append(score)

    best_k = k_range[np.argmax(k_scores)]
    best_score = max(k_scores)

    print(f"  æœ€ä½³Kå€¼ï¼š{best_k}")
    print(f"  æœ€ä½³å‡†ç¡®ç‡ï¼š{best_score:.2%}")
    print()

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(k_range, k_scores, 'bo-')
    plt.xlabel('Kå€¼')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('ä¸åŒKå€¼çš„å‡†ç¡®ç‡')
    plt.grid(True)

    output_dir = '/mnt/c/dev/python/qqstudy/ai_intro'
    plt.savefig(os.path.join(output_dir, 'iris_k_comparison.png'), dpi=100)
    print("  âœ… Kå€¼æ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°ï¼širis_k_comparison.png")
    plt.close()
    print()


def save_model(model, filename='iris_model.pkl'):
    """
    ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶

    å‚æ•°:
        model: è¦ä¿å­˜çš„æ¨¡å‹
        filename: ä¿å­˜çš„æ–‡ä»¶å

    åŠŸèƒ½:
        å°†æ¨¡å‹åºåˆ—åŒ–ä¿å­˜ï¼Œæ–¹ä¾¿åç»­åŠ è½½ä½¿ç”¨
    """
    output_dir = '/mnt/c/dev/python/qqstudy/ai_intro'
    filepath = os.path.join(output_dir, filename)
    joblib.dump(model, filepath)
    print(f"  âœ… æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{filename}")
    print()


def main():
    """
    ä¸»ç¨‹åºå…¥å£

    æ‰§è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹ï¼š
    1. åŠ è½½æ•°æ®
    2. æ¢ç´¢æ•°æ®
    3. è®­ç»ƒæ¨¡å‹
    4. è¯„ä¼°æ¨¡å‹
    5. é¢„æµ‹æ–°æ•°æ®
    6. ä¼˜åŒ–å‚æ•°
    7. ä¿å­˜æ¨¡å‹
    """
    # 1. åŠ è½½æ•°æ®
    X, y, feature_names, target_names = load_data()

    # 2. æ¢ç´¢æ•°æ®
    explore_data(X, y, feature_names, target_names)

    # 3. è®­ç»ƒæ¨¡å‹
    model, X_train, X_test, y_train, y_test = train_model(X, y)

    # 4. è¯„ä¼°æ¨¡å‹
    evaluate_model(model, X_test, y_test, target_names)

    # 5. é¢„æµ‹æ–°æ•°æ®
    predict_new(model, target_names)

    # 6. å¯»æ‰¾æœ€ä½³Kå€¼
    find_best_k(X, y)

    # 7. ä¿å­˜æ¨¡å‹
    print("ã€æ­¥éª¤7ï¼šä¿å­˜æ¨¡å‹ã€‘")
    save_model(model)

    print("=" * 50)
    print("ğŸ‰ é¡¹ç›®å®Œæˆï¼")
    print("=" * 50)
    print()
    print("ä½ å­¦ä¼šäº†ï¼š")
    print("  âœ“ åŠ è½½å’Œæ¢ç´¢æ•°æ®")
    print("  âœ“ è®­ç»ƒKNNåˆ†ç±»å™¨")
    print("  âœ“ è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("  âœ“ é¢„æµ‹æ–°æ•°æ®")
    print("  âœ“ ä¼˜åŒ–æ¨¡å‹å‚æ•°")
    print("  âœ“ ä¿å­˜å’ŒåŠ è½½æ¨¡å‹")
    print()


if __name__ == "__main__":
    main()
