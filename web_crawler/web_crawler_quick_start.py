# -*- coding: utf-8 -*-
"""
ç½‘ç»œçˆ¬è™«é€Ÿæˆæ•™ç¨‹ - é€‚åˆåˆä¸­ç”Ÿå­¦ä¹ 
=====================================

ä½œè€…: Pythonå­¦ä¹ æ•™ç¨‹
é€‚åˆäººç¾¤: åˆä¸­ç”ŸåŠç¼–ç¨‹åˆå­¦è€…
å­¦ä¹ æ—¶é—´: çº¦2-3å°æ—¶

ä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ï¼Ÿ
----------------
ç½‘ç»œçˆ¬è™«ï¼ˆWeb Crawlerï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨æµè§ˆç½‘é¡µçš„ç¨‹åºã€‚
å®ƒåƒä¸€åªå°èœ˜è››ï¼Œåœ¨äº’è”ç½‘çš„"ç½‘"ä¸Šçˆ¬è¡Œï¼Œè‡ªåŠ¨è®¿é—®ç½‘é¡µå¹¶æå–ä½ éœ€è¦çš„ä¿¡æ¯ã€‚

âš ï¸ é‡è¦æé†’ï¼šçˆ¬è™«è¦åˆæ³•åˆè§„ï¼
---------------------------
1. éµå®ˆç½‘ç«™çš„ robots.txt è§„åˆ™
2. ä¸è¦é¢‘ç¹è¯·æ±‚ï¼Œç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
3. ä¸è¦çˆ¬å–éšç§ä¿¡æ¯å’Œä»˜è´¹å†…å®¹
4. å­¦ä¹ ç›®çš„ä½¿ç”¨ï¼Œä¸è¦ç”¨äºå•†ä¸š

è®©æˆ‘ä»¬å¼€å§‹å§ï¼
"""

print("=" * 60)
print("ğŸš€ æ¬¢è¿æ¥åˆ°ç½‘ç»œçˆ¬è™«é€Ÿæˆæ•™ç¨‹ï¼")
print("=" * 60)


# ============================================================
# ç¬¬1èŠ‚ï¼šä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ï¼Ÿ
# ============================================================
"""
ğŸ“– ç¬¬1èŠ‚ï¼šä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ï¼Ÿ

ã€æ¦‚å¿µã€‘
ç½‘ç»œçˆ¬è™«ï¼ˆä¹Ÿå«ç½‘ç»œèœ˜è››ã€ç½‘ç»œæœºå™¨äººï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨è·å–ç½‘é¡µå†…å®¹çš„ç¨‹åºã€‚

ã€å·¥ä½œåŸç†ã€‘
1. å‘é€è¯·æ±‚ï¼šå‘ç½‘ç«™æœåŠ¡å™¨å‘é€HTTPè¯·æ±‚
2. è·å–å“åº”ï¼šæœåŠ¡å™¨è¿”å›ç½‘é¡µå†…å®¹ï¼ˆHTMLä»£ç ï¼‰
3. è§£æå†…å®¹ï¼šä»HTMLä¸­æå–éœ€è¦çš„æ•°æ®
4. ä¿å­˜æ•°æ®ï¼šæŠŠæ•°æ®å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“

ã€ç±»æ¯”ç†è§£ã€‘
æƒ³è±¡ä½ åœ¨å›¾ä¹¦é¦†æ‰¾èµ„æ–™ï¼š
- ä½  = çˆ¬è™«ç¨‹åº
- å›¾ä¹¦é¦† = äº’è”ç½‘
- ä¹¦æ¶ä¸Šçš„ä¹¦ = ç½‘é¡µ
- ç¿»ä¹¦æ‰¾ä¿¡æ¯ = è§£æHTML
- æŠ„å†™ç¬”è®° = ä¿å­˜æ•°æ®

ã€æ³•å¾‹ä¸é“å¾·ã€‘
âœ… å¯ä»¥åšçš„ï¼š
   - çˆ¬å–å…¬å¼€çš„ã€å…è®¸çˆ¬å–çš„æ•°æ®
   - éµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾
   - ç”¨äºå­¦ä¹ ç ”ç©¶

âŒ ä¸èƒ½åšçš„ï¼š
   - çˆ¬å–ä¸ªäººéšç§ä¿¡æ¯
   - ç»•è¿‡ä»˜è´¹å¢™çˆ¬å–ä»˜è´¹å†…å®¹
   - é¢‘ç¹è¯·æ±‚å¯¼è‡´æœåŠ¡å™¨ç˜«ç—ª
   - å‡ºå”®çˆ¬å–çš„æ•°æ®

ã€robots.txt æ˜¯ä»€ä¹ˆï¼Ÿã€‘
robots.txt æ˜¯ç½‘ç«™å‘Šè¯‰çˆ¬è™«å“ªäº›é¡µé¢å¯ä»¥çˆ¬ã€å“ªäº›ä¸èƒ½çˆ¬çš„æ–‡ä»¶ã€‚
ä¾‹å¦‚ï¼šhttps://www.baidu.com/robots.txt
"""

def section_1_what_is_crawler():
    """
    ç¬¬1èŠ‚æ¼”ç¤ºï¼šäº†è§£çˆ¬è™«çš„åŸºæœ¬æ¦‚å¿µ

    è¿™ä¸ªå‡½æ•°å±•ç¤ºäº†çˆ¬è™«çš„åŸºæœ¬å·¥ä½œæµç¨‹

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_1_what_is_crawler()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬1èŠ‚ï¼šä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ï¼Ÿ")
    print("=" * 60)

    print("""
ã€çˆ¬è™«çš„å·¥ä½œæµç¨‹ã€‘

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. å‘é€è¯·æ±‚  â”‚  â†’ å‘ç½‘ç«™æœåŠ¡å™¨å‘é€HTTPè¯·æ±‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. è·å–å“åº”  â”‚  â†’ æœåŠ¡å™¨è¿”å›HTMLä»£ç 
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. è§£æå†…å®¹  â”‚  â†’ ä»HTMLä¸­æå–æ•°æ®
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  4. ä¿å­˜æ•°æ®  â”‚  â†’ å­˜å…¥æ–‡ä»¶æˆ–æ•°æ®åº“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€é‡è¦æé†’ã€‘
- çˆ¬è™«å‰å…ˆçœ‹çœ‹ç½‘ç«™çš„ robots.txt
- è®¾ç½®è¯·æ±‚é—´éš”ï¼Œä¸è¦ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
- å°Šé‡ç‰ˆæƒï¼Œä¸è¦æ»¥ç”¨çˆ¬å–çš„æ•°æ®
    """)


# ============================================================
# ç¬¬1èŠ‚è°ƒç”¨ç¤ºä¾‹
# ============================================================
def example_section_1():
    """
    ç¬¬1èŠ‚è°ƒç”¨ç¤ºä¾‹ï¼šæ¼”ç¤ºå¦‚ä½•è°ƒç”¨ section_1_what_is_crawler()

    è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•è¿è¡Œç¬¬1èŠ‚çš„å†…å®¹
    """
    # ç›´æ¥è°ƒç”¨å‡½æ•°å³å¯
    section_1_what_is_crawler()

    # ä¹Ÿå¯ä»¥è¿™æ ·ç†è§£ï¼š
    # section_1_what_is_crawler  # è¿™æ˜¯å‡½æ•°åï¼ˆä¸è¦åŠ æ‹¬å·ï¼‰
    # section_1_what_is_crawler()  # è¿™æ˜¯è°ƒç”¨å‡½æ•°ï¼ˆè¦åŠ æ‹¬å·ï¼‰
    print("\næç¤ºï¼šsection_1 ä¸éœ€è¦ä¼ å‚æ•°ï¼Œç›´æ¥è°ƒç”¨å³å¯ï¼")


# ============================================================
# ç¬¬2èŠ‚ï¼šHTTP è¯·æ±‚åŸºç¡€
# ============================================================
"""
ğŸ“– ç¬¬2èŠ‚ï¼šHTTP è¯·æ±‚åŸºç¡€

ã€ä»€ä¹ˆæ˜¯HTTPï¼Ÿã€‘
HTTPï¼ˆè¶…æ–‡æœ¬ä¼ è¾“åè®®ï¼‰æ˜¯æµè§ˆå™¨å’ŒæœåŠ¡å™¨ä¹‹é—´é€šä¿¡çš„"è¯­è¨€"ã€‚

ã€requests åº“ã€‘
requests æ˜¯ Python æœ€æµè¡Œçš„ HTTP è¯·æ±‚åº“ï¼Œéå¸¸ç®€å•æ˜“ç”¨ã€‚

å®‰è£…æ–¹æ³•ï¼š
pip install requests

ã€å¸¸ç”¨æ–¹æ³•ã€‘
- requests.get(url)       # è·å–ç½‘é¡µ
- requests.post(url)      # æäº¤è¡¨å•
- response.status_code    # çŠ¶æ€ç 
- response.text           # ç½‘é¡µå†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
- response.content        # ç½‘é¡µå†…å®¹ï¼ˆå­—èŠ‚ï¼‰

ã€å¸¸è§çŠ¶æ€ç ã€‘
- 200: æˆåŠŸ
- 404: é¡µé¢ä¸å­˜åœ¨
- 403: ç¦æ­¢è®¿é—®
- 500: æœåŠ¡å™¨é”™è¯¯
"""

def section_2_http_basics():
    """
    ç¬¬2èŠ‚æ¼”ç¤ºï¼šå­¦ä¹ HTTPè¯·æ±‚åŸºç¡€

    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ requests åº“å‘é€HTTPè¯·æ±‚

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_2_http_basics()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬2èŠ‚ï¼šHTTP è¯·æ±‚åŸºç¡€")
    print("=" * 60)

    # å¯¼å…¥ requests åº“
    import requests

    # ç¤ºä¾‹ç½‘ç«™ï¼šhttpbin.org æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæµ‹è¯•HTTPè¯·æ±‚çš„ç½‘ç«™
    print("\nã€ç¤ºä¾‹1ã€‘å‘é€GETè¯·æ±‚")
    print("-" * 40)

    # å‘é€GETè¯·æ±‚
    url = "https://httpbin.org/get"
    print(f"æ­£åœ¨è¯·æ±‚: {url}")

    response = requests.get(url)

    # æŸ¥çœ‹çŠ¶æ€ç 
    print(f"çŠ¶æ€ç : {response.status_code}")

    # çŠ¶æ€ç å«ä¹‰
    status_meanings = {
        200: "æˆåŠŸï¼è¯·æ±‚å·²å®Œæˆ",
        404: "é¡µé¢ä¸å­˜åœ¨",
        403: "ç¦æ­¢è®¿é—®",
        500: "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
    }
    print(f"çŠ¶æ€ç å«ä¹‰: {status_meanings.get(response.status_code, 'æœªçŸ¥çŠ¶æ€')}")

    # æŸ¥çœ‹å“åº”å†…å®¹ï¼ˆå‰200ä¸ªå­—ç¬¦ï¼‰
    print(f"å“åº”å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {response.text[:200]}...")

    print("\nã€ç¤ºä¾‹2ã€‘å¸¦å‚æ•°çš„GETè¯·æ±‚")
    print("-" * 40)

    # å¾ˆå¤šç½‘ç«™éœ€è¦ä¼ é€’å‚æ•°ï¼Œæ¯”å¦‚æœç´¢
    # ä¾‹å¦‚ï¼šhttps://example.com/search?q=python
    params = {
        "q": "pythonçˆ¬è™«",
        "page": 1
    }

    response = requests.get("https://httpbin.org/get", params=params)
    print(f"å®é™…è¯·æ±‚URL: {response.url}")

    print("\nã€ç¤ºä¾‹3ã€‘æ·»åŠ è¯·æ±‚å¤´")
    print("-" * 40)

    # è¯·æ±‚å¤´å¯ä»¥å‘Šè¯‰æœåŠ¡å™¨ä½ æ˜¯è°
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    response = requests.get("https://httpbin.org/get", headers=headers)
    print(f"çŠ¶æ€ç : {response.status_code}")

    # è§£æJSONå“åº”
    data = response.json()
    print(f"æœåŠ¡å™¨æ”¶åˆ°çš„User-Agent: {data['headers']['User-Agent']}")


def send_request(url, params=None, headers=None, timeout=10):
    """
    å‘é€HTTPè¯·æ±‚çš„é€šç”¨å‡½æ•°

    è¿™æ˜¯ä¸€ä¸ªå°è£…å¥½çš„è¯·æ±‚å‡½æ•°ï¼Œå¸¦æœ‰é”™è¯¯å¤„ç†

    å‚æ•°è¯´æ˜ï¼š
        url (str): è¦è¯·æ±‚çš„ç½‘å€
        params (dict): URLå‚æ•°ï¼Œå¦‚ {"q": "python", "page": 1}
        headers (dict): è¯·æ±‚å¤´ï¼Œå¦‚ {"User-Agent": "..."}
        timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’

    è¿”å›å€¼ï¼š
        Responseå¯¹è±¡ï¼Œå¦‚æœå¤±è´¥è¿”å›None

    è°ƒç”¨ç¤ºä¾‹ï¼š
        # ç®€å•è¯·æ±‚
        response = send_request("https://httpbin.org/get")

        # å¸¦å‚æ•°è¯·æ±‚
        response = send_request(
            "https://httpbin.org/get",
            params={"q": "python"},
            headers={"User-Agent": "MyCrawler/1.0"}
        )
    """
    import requests

    try:
        response = requests.get(url, params=params, headers=headers, timeout=timeout)

        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code == 200:
            return response
        elif response.status_code == 404:
            print(f"âŒ é”™è¯¯ï¼šé¡µé¢ä¸å­˜åœ¨ (404)")
        elif response.status_code == 403:
            print(f"âŒ é”™è¯¯ï¼šç¦æ­¢è®¿é—® (403)ï¼Œå¯èƒ½éœ€è¦è®¾ç½®User-Agent")
        else:
            print(f"âŒ é”™è¯¯ï¼šçŠ¶æ€ç  {response.status_code}")
        return None

    except requests.exceptions.Timeout:
        print(f"âŒ é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡{timeout}ç§’ï¼‰")
        return None
    except requests.exceptions.ConnectionError:
        print(f"âŒ é”™è¯¯ï¼šç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
        return None
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{e}")
        return None


# ============================================================
# ç¬¬3èŠ‚ï¼šè§£æ HTML
# ============================================================
"""
ğŸ“– ç¬¬3èŠ‚ï¼šè§£æ HTML

ã€ä»€ä¹ˆæ˜¯HTMLï¼Ÿã€‘
HTMLï¼ˆè¶…æ–‡æœ¬æ ‡è®°è¯­è¨€ï¼‰æ˜¯ç½‘é¡µçš„"éª¨æ¶"ï¼Œç”¨æ¥å®šä¹‰ç½‘é¡µçš„ç»“æ„ã€‚

ã€HTMLåŸºæœ¬ç»“æ„ã€‘
<html>
    <head>
        <title>ç½‘é¡µæ ‡é¢˜</title>
    </head>
    <body>
        <h1>è¿™æ˜¯ä¸€çº§æ ‡é¢˜</h1>
        <p>è¿™æ˜¯ä¸€ä¸ªæ®µè½</p>
        <a href="https://example.com">è¿™æ˜¯ä¸€ä¸ªé“¾æ¥</a>
        <div class="container">
            <span id="price">99å…ƒ</span>
        </div>
    </body>
</html>

ã€BeautifulSoup åº“ã€‘
BeautifulSoup æ˜¯æœ€æµè¡Œçš„HTMLè§£æåº“ï¼Œå¯ä»¥è½»æ¾æå–ç½‘é¡µå†…å®¹ã€‚

å®‰è£…æ–¹æ³•ï¼š
pip install beautifulsoup4

ã€å¸¸ç”¨æ–¹æ³•ã€‘
- soup.find('tag')          # æ‰¾ç¬¬ä¸€ä¸ªæ ‡ç­¾
- soup.find_all('tag')      # æ‰¾æ‰€æœ‰æ ‡ç­¾
- soup.select('selector')   # CSSé€‰æ‹©å™¨
- element.text              # è·å–æ–‡æœ¬å†…å®¹
- element['attr']           # è·å–å±æ€§å€¼
"""

def section_3_parse_html():
    """
    ç¬¬3èŠ‚æ¼”ç¤ºï¼šå­¦ä¹ HTMLè§£æ

    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ BeautifulSoup è§£æHTML

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_3_parse_html()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬3èŠ‚ï¼šè§£æ HTML")
    print("=" * 60)

    from bs4 import BeautifulSoup

    # ç¤ºä¾‹HTMLä»£ç ï¼ˆæ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„ç½‘é¡µï¼‰
    html = """
    <html>
        <head>
            <title>æˆ‘çš„ç¬¬ä¸€ä¸ªç½‘é¡µ</title>
        </head>
        <body>
            <h1 class="main-title">æ¬¢è¿å­¦ä¹ çˆ¬è™«</h1>
            <div class="content">
                <p class="intro">è¿™æ˜¯ä¸€ä¸ªç”¨äºå­¦ä¹ çš„ç¤ºä¾‹ç½‘é¡µã€‚</p>
                <p class="description">çˆ¬è™«å¾ˆæœ‰è¶£ï¼</p>
            </div>
            <ul class="book-list">
                <li class="book">
                    <span class="title">Pythonå…¥é—¨</span>
                    <span class="price">59å…ƒ</span>
                </li>
                <li class="book">
                    <span class="title">çˆ¬è™«å®æˆ˜</span>
                    <span class="price">79å…ƒ</span>
                </li>
                <li class="book">
                    <span class="title">æ•°æ®åˆ†æ</span>
                    <span class="price">89å…ƒ</span>
                </li>
            </ul>
            <a href="https://example.com/page1">ç¬¬1é¡µ</a>
            <a href="https://example.com/page2">ç¬¬2é¡µ</a>
        </body>
    </html>
    """

    print("\nã€ç¤ºä¾‹1ã€‘åˆ›å»ºBeautifulSoupå¯¹è±¡")
    print("-" * 40)

    # åˆ›å»ºBeautifulSoupå¯¹è±¡
    # 'html.parser' æ˜¯Pythonå†…ç½®çš„è§£æå™¨
    soup = BeautifulSoup(html, 'html.parser')

    print(f"ç½‘é¡µæ ‡é¢˜: {soup.title.text}")

    print("\nã€ç¤ºä¾‹2ã€‘find() - æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ ‡ç­¾")
    print("-" * 40)

    # find() åªè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ 
    first_p = soup.find('p')
    print(f"ç¬¬ä¸€ä¸ª<p>æ ‡ç­¾: {first_p.text}")

    # å¯ä»¥æŒ‰classæŸ¥æ‰¾
    intro = soup.find('p', class_='intro')
    print(f"classä¸ºintroçš„<p>: {intro.text}")

    # å¯ä»¥æŒ‰idæŸ¥æ‰¾
    # element = soup.find('tag', id='my-id')

    print("\nã€ç¤ºä¾‹3ã€‘find_all() - æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ ‡ç­¾")
    print("-" * 40)

    # find_all() è¿”å›æ‰€æœ‰åŒ¹é…çš„å…ƒç´ ï¼ˆåˆ—è¡¨ï¼‰
    all_p = soup.find_all('p')
    print(f"æ‰¾åˆ°{len(all_p)}ä¸ª<p>æ ‡ç­¾:")
    for i, p in enumerate(all_p, 1):
        print(f"  {i}. {p.text}")

    # æŸ¥æ‰¾æ‰€æœ‰ä¹¦ç±
    books = soup.find_all('li', class_='book')
    print(f"\næ‰¾åˆ°{len(books)}æœ¬ä¹¦:")
    for book in books:
        title = book.find('span', class_='title').text
        price = book.find('span', class_='price').text
        print(f"  - {title}: {price}")

    print("\nã€ç¤ºä¾‹4ã€‘CSSé€‰æ‹©å™¨ - select() å’Œ select_one()")
    print("-" * 40)

    # select_one() è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ 
    first_link = soup.select_one('a')
    print(f"ç¬¬ä¸€ä¸ªé“¾æ¥: {first_link.text} -> {first_link['href']}")

    # select() è¿”å›æ‰€æœ‰åŒ¹é…çš„å…ƒç´ 
    all_links = soup.select('a')
    print(f"\næ‰€æœ‰é“¾æ¥:")
    for link in all_links:
        print(f"  - {link.text}: {link['href']}")

    # æ›´å¤æ‚çš„é€‰æ‹©å™¨
    book_titles = soup.select('.book-list .book .title')
    print(f"\næ‰€æœ‰ä¹¦åï¼ˆCSSé€‰æ‹©å™¨ï¼‰:")
    for title in book_titles:
        print(f"  - {title.text}")


def parse_html(html_content, parser='html.parser'):
    """
    è§£æHTMLå†…å®¹çš„é€šç”¨å‡½æ•°

    å‚æ•°è¯´æ˜ï¼š
        html_content (str): HTMLå­—ç¬¦ä¸²
        parser (str): è§£æå™¨ï¼Œé»˜è®¤ 'html.parser'

    è¿”å›å€¼ï¼š
        BeautifulSoupå¯¹è±¡

    è°ƒç”¨ç¤ºä¾‹ï¼š
        soup = parse_html("<html><body>Hello</body></html>")
        print(soup.body.text)
    """
    from bs4 import BeautifulSoup
    return BeautifulSoup(html_content, parser)


# ============================================================
# ç¬¬4èŠ‚ï¼šæå–æ•°æ®
# ============================================================
"""
ğŸ“– ç¬¬4èŠ‚ï¼šæå–æ•°æ®

ã€æå–æ–‡æœ¬ã€‘
element.text          # è·å–æ ‡ç­¾å†…çš„çº¯æ–‡æœ¬
element.string        # è·å–å”¯ä¸€å­—ç¬¦ä¸²ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªå­—ç¬¦ä¸²ï¼‰
element.get_text()    # è·å–æ‰€æœ‰æ–‡æœ¬ï¼Œå¯è®¾ç½®åˆ†éš”ç¬¦

ã€æå–å±æ€§ã€‘
element['href']       # è·å–hrefå±æ€§
element['class']      # è·å–classå±æ€§ï¼ˆè¿”å›åˆ—è¡¨ï¼‰
element.get('href')   # å®‰å…¨è·å–ï¼Œä¸å­˜åœ¨è¿”å›None

ã€å¯¼èˆªã€‘
element.parent        # çˆ¶å…ƒç´ 
element.children      # æ‰€æœ‰å­å…ƒç´ 
element.next_sibling  # ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
element.prev_sibling  # ä¸Šä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
"""

def section_4_extract_data():
    """
    ç¬¬4èŠ‚æ¼”ç¤ºï¼šå­¦ä¹ æ•°æ®æå–

    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºå¦‚ä½•ä»HTMLä¸­æå–å„ç§æ•°æ®

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_4_extract_data()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬4èŠ‚ï¼šæå–æ•°æ®")
    print("=" * 60)

    from bs4 import BeautifulSoup

    # æ›´å¤æ‚çš„HTMLç¤ºä¾‹
    html = """
    <div class="product-card">
        <h2 class="name">Pythonç¼–ç¨‹ä¹¦</h2>
        <p class="description">é€‚åˆåˆå­¦è€…çš„Pythonæ•™ç¨‹</p>
        <div class="price-info">
            <span class="original-price">Â¥99.00</span>
            <span class="sale-price">Â¥59.00</span>
        </div>
        <a href="/product/123" class="buy-link" data-id="123">ç«‹å³è´­ä¹°</a>
        <img src="/images/book.jpg" alt="Pythonç¼–ç¨‹ä¹¦å°é¢">
        <ul class="tags">
            <li>ç¼–ç¨‹</li>
            <li>Python</li>
            <li>å…¥é—¨</li>
        </ul>
    </div>
    """

    soup = BeautifulSoup(html, 'html.parser')

    print("\nã€ç¤ºä¾‹1ã€‘æå–æ–‡æœ¬å†…å®¹")
    print("-" * 40)

    # æ‰¾åˆ°å•†å“å¡ç‰‡
    card = soup.find('div', class_='product-card')

    # æå–å•†å“å
    name = card.find('h2', class_='name').text
    print(f"å•†å“å: {name}")

    # æå–æè¿°
    desc = card.find('p', class_='description').text
    print(f"æè¿°: {desc}")

    # æå–æ‰€æœ‰æ ‡ç­¾
    tags = card.find('ul', class_='tags').find_all('li')
    print(f"æ ‡ç­¾: {', '.join([tag.text for tag in tags])}")

    print("\nã€ç¤ºä¾‹2ã€‘æå–å±æ€§å€¼")
    print("-" * 40)

    # æå–é“¾æ¥åœ°å€
    link = card.find('a', class_='buy-link')
    href = link['href']
    data_id = link['data-id']
    print(f"é“¾æ¥åœ°å€: {href}")
    print(f"æ•°æ®ID: {data_id}")

    # æå–å›¾ç‰‡åœ°å€
    img = card.find('img')
    img_src = img['src']
    img_alt = img['alt']
    print(f"å›¾ç‰‡åœ°å€: {img_src}")
    print(f"å›¾ç‰‡æè¿°: {img_alt}")

    print("\nã€ç¤ºä¾‹3ã€‘å®‰å…¨æå–ï¼ˆé¿å…æŠ¥é”™ï¼‰")
    print("-" * 40)

    # ä½¿ç”¨ get() æ–¹æ³•ï¼Œå¦‚æœå±æ€§ä¸å­˜åœ¨ä¸ä¼šæŠ¥é”™
    link = card.find('a')
    href = link.get('href', 'æ— ')  # å¦‚æœæ²¡æœ‰hrefï¼Œè¿”å›'æ— '
    target = link.get('target', '_self')  # å¦‚æœæ²¡æœ‰targetï¼Œè¿”å›'_self'
    print(f"href: {href}")
    print(f"target: {target}")

    # å®‰å…¨æå–æ–‡æœ¬
    price_element = card.find('span', class_='sale-price')
    price = price_element.text if price_element else "ä»·æ ¼æœªçŸ¥"
    print(f"å”®ä»·: {price}")

    print("\nã€ç¤ºä¾‹4ã€‘æå–åµŒå¥—æ•°æ®")
    print("-" * 40)

    # æå–ä»·æ ¼ä¿¡æ¯
    price_info = card.find('div', class_='price-info')
    original_price = price_info.find('span', class_='original-price').text
    sale_price = price_info.find('span', class_='sale-price').text
    print(f"åŸä»·: {original_price}")
    print(f"å”®ä»·: {sale_price}")

    # è®¡ç®—æŠ˜æ‰£
    import re
    orig = float(re.search(r'[\d.]+', original_price).group())
    sale = float(re.search(r'[\d.]+', sale_price).group())
    discount = sale / orig * 10
    print(f"æŠ˜æ‰£: {discount:.1f}æŠ˜")


def extract_text(element, default='æ— '):
    """
    å®‰å…¨æå–å…ƒç´ çš„æ–‡æœ¬å†…å®¹

    å‚æ•°è¯´æ˜ï¼š
        element: BeautifulSoupå…ƒç´ 
        default (str): å¦‚æœå…ƒç´ ä¸å­˜åœ¨ï¼Œè¿”å›çš„é»˜è®¤å€¼

    è¿”å›å€¼ï¼š
        str: æ–‡æœ¬å†…å®¹æˆ–é»˜è®¤å€¼

    è°ƒç”¨ç¤ºä¾‹ï¼š
        text = extract_text(soup.find('h1'))
        text = extract_text(soup.find('h1'), default='æœªæ‰¾åˆ°')
    """
    if element:
        return element.get_text(strip=True)
    return default


def extract_attribute(element, attr, default=None):
    """
    å®‰å…¨æå–å…ƒç´ çš„å±æ€§å€¼

    å‚æ•°è¯´æ˜ï¼š
        element: BeautifulSoupå…ƒç´ 
        attr (str): å±æ€§å
        default: å¦‚æœå±æ€§ä¸å­˜åœ¨ï¼Œè¿”å›çš„é»˜è®¤å€¼

    è¿”å›å€¼ï¼š
        å±æ€§å€¼æˆ–é»˜è®¤å€¼

    è°ƒç”¨ç¤ºä¾‹ï¼š
        href = extract_attribute(link, 'href')
        href = extract_attribute(link, 'href', default='#')
    """
    if element:
        return element.get(attr, default)
    return default


# ============================================================
# ç¬¬5èŠ‚ï¼šä¿å­˜æ•°æ®
# ============================================================
"""
ğŸ“– ç¬¬5èŠ‚ï¼šä¿å­˜æ•°æ®

ã€å¸¸è§çš„ä¿å­˜æ–¹å¼ã€‘
1. CSVæ–‡ä»¶ - è¡¨æ ¼æ•°æ®ï¼Œå¯ç”¨Excelæ‰“å¼€
2. JSONæ–‡ä»¶ - ç»“æ„åŒ–æ•°æ®ï¼Œæ˜“äºäº¤æ¢
3. TXTæ–‡ä»¶ - ç®€å•æ–‡æœ¬
4. æ•°æ®åº“ - å¤§é‡æ•°æ®ï¼Œå¦‚SQLite

ã€CSVæ ¼å¼ã€‘
é€—å·åˆ†éš”å€¼ï¼Œä¾‹å¦‚ï¼š
å§“å,å¹´é¾„,åŸå¸‚
å¼ ä¸‰,18,åŒ—äº¬
æå››,20,ä¸Šæµ·

ã€JSONæ ¼å¼ã€‘
JavaScriptå¯¹è±¡è¡¨ç¤ºæ³•ï¼Œä¾‹å¦‚ï¼š
{
    "name": "å¼ ä¸‰",
    "age": 18,
    "city": "åŒ—äº¬"
}
"""

def section_5_save_data():
    """
    ç¬¬5èŠ‚æ¼”ç¤ºï¼šå­¦ä¹ ä¿å­˜æ•°æ®

    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºå¦‚ä½•å°†çˆ¬å–çš„æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_5_save_data()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬5èŠ‚ï¼šä¿å­˜æ•°æ®")
    print("=" * 60)

    import csv
    import json
    import os

    # æ¨¡æ‹Ÿçˆ¬å–çš„æ•°æ®
    books = [
        {"title": "Pythonå…¥é—¨", "author": "å¼ ä¸‰", "price": 59},
        {"title": "çˆ¬è™«å®æˆ˜", "author": "æå››", "price": 79},
        {"title": "æ•°æ®åˆ†æ", "author": "ç‹äº”", "price": 89},
    ]

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = "/mnt/c/dev/python/qqstudy/web_crawler/data"
    os.makedirs(save_dir, exist_ok=True)

    print("\nã€ç¤ºä¾‹1ã€‘ä¿å­˜ä¸ºCSVæ–‡ä»¶")
    print("-" * 40)

    csv_file = os.path.join(save_dir, "books.csv")

    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        # åˆ›å»ºå†™å…¥å™¨
        writer = csv.writer(f)

        # å†™å…¥è¡¨å¤´
        writer.writerow(['ä¹¦å', 'ä½œè€…', 'ä»·æ ¼'])

        # å†™å…¥æ•°æ®
        for book in books:
            writer.writerow([book['title'], book['author'], book['price']])

    print(f"âœ… å·²ä¿å­˜åˆ°: {csv_file}")

    # ä½¿ç”¨DictWriterï¼ˆæ›´æ–¹ä¾¿ï¼‰
    csv_file2 = os.path.join(save_dir, "books_dict.csv")

    with open(csv_file2, 'w', newline='', encoding='utf-8') as f:
        # åˆ›å»ºå­—å…¸å†™å…¥å™¨
        writer = csv.DictWriter(f, fieldnames=['title', 'author', 'price'])

        # å†™å…¥è¡¨å¤´
        writer.writeheader()

        # å†™å…¥æ•°æ®
        writer.writerows(books)

    print(f"âœ… å·²ä¿å­˜åˆ°: {csv_file2}")

    print("\nã€ç¤ºä¾‹2ã€‘ä¿å­˜ä¸ºJSONæ–‡ä»¶")
    print("-" * 40)

    json_file = os.path.join(save_dir, "books.json")

    with open(json_file, 'w', encoding='utf-8') as f:
        # ensure_ascii=False ä¿è¯ä¸­æ–‡æ­£å¸¸æ˜¾ç¤º
        # indent=4 æ ¼å¼åŒ–è¾“å‡ºï¼Œä¾¿äºé˜…è¯»
        json.dump(books, f, ensure_ascii=False, indent=4)

    print(f"âœ… å·²ä¿å­˜åˆ°: {json_file}")

    # è¯»å–JSONæ–‡ä»¶
    with open(json_file, 'r', encoding='utf-8') as f:
        loaded_data = json.load(f)

    print(f"è¯»å–åˆ°çš„æ•°æ®: {loaded_data[0]}")

    print("\nã€ç¤ºä¾‹3ã€‘ä¿å­˜ä¸ºTXTæ–‡ä»¶")
    print("-" * 40)

    txt_file = os.path.join(save_dir, "books.txt")

    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("=" * 40 + "\n")
        f.write("ä¹¦ç±åˆ—è¡¨\n")
        f.write("=" * 40 + "\n\n")

        for i, book in enumerate(books, 1):
            f.write(f"{i}. {book['title']}\n")
            f.write(f"   ä½œè€…: {book['author']}\n")
            f.write(f"   ä»·æ ¼: Â¥{book['price']}\n\n")

    print(f"âœ… å·²ä¿å­˜åˆ°: {txt_file}")

    print("\nã€ç¤ºä¾‹4ã€‘ä¿å­˜åˆ°SQLiteæ•°æ®åº“")
    print("-" * 40)

    import sqlite3

    db_file = os.path.join(save_dir, "books.db")

    # è¿æ¥æ•°æ®åº“ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # åˆ›å»ºè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS books (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            author TEXT,
            price REAL
        )
    ''')

    # æ’å…¥æ•°æ®
    for book in books:
        cursor.execute(
            'INSERT INTO books (title, author, price) VALUES (?, ?, ?)',
            (book['title'], book['author'], book['price'])
        )

    # æäº¤äº‹åŠ¡
    conn.commit()

    # æŸ¥è¯¢æ•°æ®
    cursor.execute('SELECT * FROM books')
    all_books = cursor.fetchall()
    print(f"æ•°æ®åº“ä¸­çš„ä¹¦ç±:")
    for book in all_books:
        print(f"  {book}")

    # å…³é—­è¿æ¥
    conn.close()

    print(f"âœ… å·²ä¿å­˜åˆ°: {db_file}")


def save_to_csv(data, filepath, fieldnames=None):
    """
    ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶

    å‚æ•°è¯´æ˜ï¼š
        data (list): æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸
        filepath (str): ä¿å­˜è·¯å¾„
        fieldnames (list): å­—æ®µååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—å…¸çš„é”®

    è¿”å›å€¼ï¼š
        bool: æ˜¯å¦æˆåŠŸ

    è°ƒç”¨ç¤ºä¾‹ï¼š
        data = [
            {"name": "å¼ ä¸‰", "age": 18},
            {"name": "æå››", "age": 20}
        ]
        save_to_csv(data, "output.csv")
    """
    import csv

    if not data:
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
        return False

    try:
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            if fieldnames is None:
                fieldnames = list(data[0].keys())

            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)

        print(f"âœ… å·²ä¿å­˜ {len(data)} æ¡æ•°æ®åˆ°: {filepath}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False


def save_to_json(data, filepath, indent=4):
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶

    å‚æ•°è¯´æ˜ï¼š
        data: è¦ä¿å­˜çš„æ•°æ®ï¼ˆåˆ—è¡¨æˆ–å­—å…¸ï¼‰
        filepath (str): ä¿å­˜è·¯å¾„
        indent (int): ç¼©è¿›ç©ºæ ¼æ•°ï¼ŒNoneè¡¨ç¤ºå‹ç¼©

    è¿”å›å€¼ï¼š
        bool: æ˜¯å¦æˆåŠŸ

    è°ƒç”¨ç¤ºä¾‹ï¼š
        data = {"name": "å¼ ä¸‰", "age": 18}
        save_to_json(data, "output.json")
    """
    import json

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=indent)

        print(f"âœ… å·²ä¿å­˜æ•°æ®åˆ°: {filepath}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False


# ============================================================
# ç¬¬6èŠ‚ï¼šå¤„ç†åçˆ¬è™«
# ============================================================
"""
ğŸ“– ç¬¬6èŠ‚ï¼šå¤„ç†åçˆ¬è™«

ã€ä¸ºä»€ä¹ˆæœ‰åçˆ¬è™«ï¼Ÿã€‘
ç½‘ç«™ä¸ºäº†ä¿æŠ¤æœåŠ¡å™¨å’Œé˜²æ­¢æ•°æ®è¢«æ»¥ç”¨ï¼Œä¼šé‡‡å–ä¸€äº›æªæ–½é˜»æ­¢çˆ¬è™«ã€‚

ã€å¸¸è§çš„åçˆ¬è™«æªæ–½ã€‘
1. æ£€æŸ¥User-Agent - è¯†åˆ«æ˜¯å¦æ˜¯æµè§ˆå™¨
2. æ£€æŸ¥Referer - åˆ¤æ–­è¯·æ±‚æ¥æº
3. IPé™åˆ¶ - åŒä¸€IPè¯·æ±‚æ¬¡æ•°è¿‡å¤š
4. éªŒè¯ç  - éœ€è¦äººå·¥éªŒè¯
5. ç™»å½•éªŒè¯ - éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®

ã€åº”å¯¹æ–¹æ³•ã€‘
1. è®¾ç½®User-Agent - æ¨¡æ‹Ÿæµè§ˆå™¨
2. æ·»åŠ Referer - æ¨¡æ‹Ÿä»ç½‘ç«™å†…éƒ¨è®¿é—®
3. è®¾ç½®è¯·æ±‚å»¶è¿Ÿ - é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
4. ä½¿ç”¨ä»£ç†IP - æ›´æ¢IPåœ°å€
5. å¤„ç†Cookie - ä¿æŒç™»å½•çŠ¶æ€

ã€é‡è¦æé†’ã€‘
âš ï¸ è¿™äº›æ–¹æ³•ä»…ç”¨äºå­¦ä¹ ï¼Œè¯·å‹¿ç”¨äºæ¶æ„çˆ¬å–ï¼
"""

def section_6_anti_crawler():
    """
    ç¬¬6èŠ‚æ¼”ç¤ºï¼šå­¦ä¹ å¤„ç†åçˆ¬è™«

    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºå¦‚ä½•è®¾ç½®è¯·æ±‚å¤´æ¥æ¨¡æ‹Ÿæµè§ˆå™¨

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_6_anti_crawler()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬6èŠ‚ï¼šå¤„ç†åçˆ¬è™«")
    print("=" * 60)

    import requests
    import time
    import random

    print("\nã€ç¤ºä¾‹1ã€‘è®¾ç½®User-Agent")
    print("-" * 40)

    # å¸¸ç”¨çš„User-Agent
    user_agents = [
        # Chromeæµè§ˆå™¨
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        # Firefoxæµè§ˆå™¨
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        # Safariæµè§ˆå™¨
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        # Edgeæµè§ˆå™¨
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    ]

    # éšæœºé€‰æ‹©ä¸€ä¸ªUser-Agent
    headers = {
        "User-Agent": random.choice(user_agents)
    }

    print(f"User-Agent: {headers['User-Agent'][:50]}...")

    response = requests.get("https://httpbin.org/get", headers=headers)
    print(f"çŠ¶æ€ç : {response.status_code}")

    print("\nã€ç¤ºä¾‹2ã€‘å®Œæ•´çš„è¯·æ±‚å¤´è®¾ç½®")
    print("-" * 40)

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://www.google.com/",  # æ¨¡æ‹Ÿä»Googleæœç´¢è¿‡æ¥
        "Connection": "keep-alive",
    }

    print("è¯·æ±‚å¤´è®¾ç½®:")
    for key, value in headers.items():
        print(f"  {key}: {value[:40]}..." if len(value) > 40 else f"  {key}: {value}")

    print("\nã€ç¤ºä¾‹3ã€‘è®¾ç½®è¯·æ±‚å»¶è¿Ÿ")
    print("-" * 40)

    # æ¨¡æ‹Ÿçˆ¬å–å¤šä¸ªé¡µé¢
    urls = [
        "https://httpbin.org/get?page=1",
        "https://httpbin.org/get?page=2",
        "https://httpbin.org/get?page=3",
    ]

    for i, url in enumerate(urls, 1):
        print(f"æ­£åœ¨è¯·æ±‚ç¬¬{i}é¡µ: {url}")

        # æ¨¡æ‹Ÿè¯·æ±‚
        # response = requests.get(url, headers=headers)

        # è®¾ç½®éšæœºå»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰
        delay = random.uniform(1, 3)
        print(f"  ç­‰å¾… {delay:.1f} ç§’...")
        # time.sleep(delay)  # å®é™…ä½¿ç”¨æ—¶å–æ¶ˆæ³¨é‡Š

        print(f"  âœ… å®Œæˆ")
        print()

    print("âš ï¸ æç¤ºï¼šè®¾ç½®å»¶è¿Ÿå¯ä»¥é¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›")

    print("\nã€ç¤ºä¾‹4ã€‘Sessionä¿æŒä¼šè¯")
    print("-" * 40)

    # ä½¿ç”¨Sessionå¯ä»¥ä¿æŒCookie
    session = requests.Session()

    # è®¾ç½®Sessionçš„é»˜è®¤headers
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    })

    # æ‰€æœ‰è¯·æ±‚éƒ½ä¼šå¸¦ä¸ŠCookie
    # response1 = session.get("https://example.com/login")
    # response2 = session.get("https://example.com/profile")

    print("Sessionå¯ä»¥ä¿æŒCookieï¼Œé€‚åˆéœ€è¦ç™»å½•çš„ç½‘ç«™")


def get_random_headers():
    """
    è·å–éšæœºè¯·æ±‚å¤´

    è¿”å›å€¼ï¼š
        dict: åŒ…å«éšæœºUser-Agentçš„è¯·æ±‚å¤´å­—å…¸

    è°ƒç”¨ç¤ºä¾‹ï¼š
        headers = get_random_headers()
        response = requests.get(url, headers=headers)
    """
    import random

    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    ]

    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }


def crawl_with_delay(url, delay_range=(1, 3), max_retries=3):
    """
    å¸¦å»¶è¿Ÿå’Œé‡è¯•çš„çˆ¬å–å‡½æ•°

    å‚æ•°è¯´æ˜ï¼š
        url (str): è¦çˆ¬å–çš„URL
        delay_range (tuple): å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰ï¼Œå¦‚ (1, 3) è¡¨ç¤º1-3ç§’
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°

    è¿”å›å€¼ï¼š
        Responseå¯¹è±¡æˆ–None

    è°ƒç”¨ç¤ºä¾‹ï¼š
        response = crawl_with_delay("https://example.com", delay_range=(2, 5))
    """
    import requests
    import time
    import random

    headers = get_random_headers()

    for attempt in range(max_retries):
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            delay = random.uniform(*delay_range)
            time.sleep(delay)

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                return response
            elif response.status_code == 429:
                # è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                wait_time = (attempt + 1) * 5
                print(f"è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"çŠ¶æ€ç  {response.status_code}ï¼Œå°è¯• {attempt + 1}/{max_retries}")

        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}ï¼Œå°è¯• {attempt + 1}/{max_retries}")

    return None


# ============================================================
# ç¬¬7èŠ‚ï¼šç¿»é¡µçˆ¬å–
# ============================================================
"""
ğŸ“– ç¬¬7èŠ‚ï¼šç¿»é¡µçˆ¬å–

ã€ä¸ºä»€ä¹ˆè¦ç¿»é¡µï¼Ÿã€‘
å¾ˆå¤šç½‘ç«™çš„å†…å®¹åˆ†å¸ƒåœ¨å¤šä¸ªé¡µé¢ï¼Œéœ€è¦ç¿»é¡µæ‰èƒ½è·å–å…¨éƒ¨æ•°æ®ã€‚

ã€ç¿»é¡µçš„ä¸¤ç§æ–¹å¼ã€‘
1. URLè§„å¾‹ç¿»é¡µ - é¡µç ç›´æ¥ä½“ç°åœ¨URLä¸­
   ä¾‹å¦‚ï¼š/page/1, /page/2, /page/3...

2. å‚æ•°ç¿»é¡µ - é€šè¿‡URLå‚æ•°æ§åˆ¶é¡µç 
   ä¾‹å¦‚ï¼š?page=1, ?page=2, ?page=3...

ã€ç¿»é¡µçˆ¬å–çš„æ­¥éª¤ã€‘
1. åˆ†æç¿»é¡µè§„å¾‹
2. æ„é€ æ¯ä¸€é¡µçš„URL
3. å¾ªç¯è¯·æ±‚æ¯ä¸€é¡µ
4. æå–æ•°æ®å¹¶ä¿å­˜
5. åˆ¤æ–­æ˜¯å¦åˆ°è¾¾æœ€åä¸€é¡µ
"""

def section_7_pagination():
    """
    ç¬¬7èŠ‚æ¼”ç¤ºï¼šå­¦ä¹ ç¿»é¡µçˆ¬å–

    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºå¦‚ä½•çˆ¬å–å¤šé¡µæ•°æ®

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_7_pagination()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬7èŠ‚ï¼šç¿»é¡µçˆ¬å–")
    print("=" * 60)

    print("\nã€ç¤ºä¾‹1ã€‘åˆ†æç¿»é¡µè§„å¾‹")
    print("-" * 40)

    # ä»¥ quotes.toscrape.com ä¸ºä¾‹
    # ç¬¬1é¡µ: https://quotes.toscrape.com/page/1/
    # ç¬¬2é¡µ: https://quotes.toscrape.com/page/2/
    # ç¬¬3é¡µ: https://quotes.toscrape.com/page/3/
    # ...

    base_url = "https://quotes.toscrape.com/page/{}/"

    print("URLè§„å¾‹åˆ†æ:")
    for page in range(1, 4):
        url = base_url.format(page)
        print(f"  ç¬¬{page}é¡µ: {url}")

    print("\nã€ç¤ºä¾‹2ã€‘å‚æ•°ç¿»é¡µ")
    print("-" * 40)

    # æœ‰äº›ç½‘ç«™ä½¿ç”¨å‚æ•°ç¿»é¡µ
    # ç¬¬1é¡µ: https://example.com/articles?page=1
    # ç¬¬2é¡µ: https://example.com/articles?page=2

    import requests

    base_url = "https://httpbin.org/get"

    print("å‚æ•°ç¿»é¡µç¤ºä¾‹:")
    for page in range(1, 4):
        params = {"page": page, "size": 10}
        # å®é™…è¯·æ±‚
        # response = requests.get(base_url, params=params)

        # æ„é€ çš„URLç¤ºä¾‹
        print(f"  ç¬¬{page}é¡µå‚æ•°: {params}")

    print("\nã€ç¤ºä¾‹3ã€‘å®Œæ•´çš„ç¿»é¡µçˆ¬å–æµç¨‹")
    print("-" * 40)

    # ç¿»é¡µçˆ¬å–çš„ä¼ªä»£ç 
    print("""
ç¿»é¡µçˆ¬å–çš„ä¸€èˆ¬æµç¨‹:

1. è®¾ç½®èµ·å§‹é¡µç å’Œæœ€å¤§é¡µæ•°
2. while å¾ªç¯:
   a. æ„é€ å½“å‰é¡µURL
   b. å‘é€è¯·æ±‚
   c. è§£æé¡µé¢æå–æ•°æ®
   d. ä¿å­˜æ•°æ®
   e. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
   f. å¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µï¼Œé€€å‡ºå¾ªç¯
   g. é¡µç  + 1
   h. æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
    """)


def crawl_multiple_pages(base_url, max_pages=5, delay=(1, 2)):
    """
    ç¿»é¡µçˆ¬å–é€šç”¨å‡½æ•°

    å‚æ•°è¯´æ˜ï¼š
        base_url (str): åŸºç¡€URLï¼Œä½¿ç”¨ {} ä½œä¸ºé¡µç å ä½ç¬¦
                        ä¾‹å¦‚ï¼š"https://example.com/page/{}"
        max_pages (int): æœ€å¤§çˆ¬å–é¡µæ•°
        delay (tuple): è¯·æ±‚å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰

    è¿”å›å€¼ï¼š
        list: æ‰€æœ‰é¡µé¢æå–çš„æ•°æ®åˆ—è¡¨

    è°ƒç”¨ç¤ºä¾‹ï¼š
        # çˆ¬å–å‰5é¡µ
        data = crawl_multiple_pages("https://quotes.toscrape.com/page/{}/", max_pages=5)

        # å¸¦è‡ªå®šä¹‰å»¶è¿Ÿ
        data = crawl_multiple_pages("https://example.com/page/{}/", max_pages=10, delay=(2, 4))
    """
    import requests
    from bs4 import BeautifulSoup
    import time
    import random

    all_data = []

    for page in range(1, max_pages + 1):
        url = base_url.format(page)
        print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(*delay))

            # å‘é€è¯·æ±‚
            headers = get_random_headers()
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                # è¿™é‡Œè¿”å›soupï¼Œå…·ä½“æå–é€»è¾‘ç”±è°ƒç”¨è€…å®ç°
                all_data.append({
                    'page': page,
                    'url': url,
                    'soup': soup
                })
                print(f"  âœ… ç¬¬ {page} é¡µçˆ¬å–æˆåŠŸ")
            else:
                print(f"  âŒ ç¬¬ {page} é¡µçŠ¶æ€ç : {response.status_code}")
                break

        except Exception as e:
            print(f"  âŒ ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥: {e}")
            break

    print(f"\nå…±çˆ¬å– {len(all_data)} é¡µ")
    return all_data


# ============================================================
# ç¬¬8èŠ‚ï¼šç»¼åˆæ¡ˆä¾‹
# ============================================================
"""
ğŸ“– ç¬¬8èŠ‚ï¼šç»¼åˆæ¡ˆä¾‹

ç°åœ¨æˆ‘ä»¬æ¥å®ç°ä¸€ä¸ªå®Œæ•´çš„çˆ¬è™«é¡¹ç›®ï¼

ã€ç›®æ ‡ã€‘
çˆ¬å– http://quotes.toscrape.com ç½‘ç«™çš„åè¨€è­¦å¥

ã€æ•°æ®ã€‘
- åè¨€å†…å®¹
- ä½œè€…
- æ ‡ç­¾

ã€æ­¥éª¤ã€‘
1. å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µ
2. è§£æHTMLæå–æ•°æ®
3. ç¿»é¡µè·å–æ›´å¤šæ•°æ®
4. ä¿å­˜åˆ°CSVæ–‡ä»¶
"""

def section_8_complete_example():
    """
    ç¬¬8èŠ‚ï¼šç»¼åˆæ¡ˆä¾‹ - çˆ¬å–åè¨€ç½‘ç«™

    è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„çˆ¬è™«ç¤ºä¾‹ï¼Œå±•ç¤ºäº†æ‰€æœ‰å‰é¢å­¦åˆ°çš„çŸ¥è¯†

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        section_8_complete_example()
    """
    print("\n" + "=" * 60)
    print("ğŸ“– ç¬¬8èŠ‚ï¼šç»¼åˆæ¡ˆä¾‹ - çˆ¬å–åè¨€ç½‘ç«™")
    print("=" * 60)

    import requests
    from bs4 import BeautifulSoup
    import csv
    import time
    import random
    import os

    # ç›®æ ‡ç½‘ç«™
    base_url = "https://quotes.toscrape.com/page/{}/"

    # å­˜å‚¨æ‰€æœ‰åè¨€
    all_quotes = []

    # çˆ¬å–å‰3é¡µä½œä¸ºç¤ºä¾‹
    max_pages = 3

    print(f"\nå¼€å§‹çˆ¬å– {base_url} å‰ {max_pages} é¡µçš„åè¨€...")
    print("-" * 40)

    for page in range(1, max_pages + 1):
        url = base_url.format(page)
        print(f"\næ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

        try:
            # 1. æ·»åŠ å»¶è¿Ÿ
            delay = random.uniform(1, 2)
            time.sleep(delay)

            # 2. å‘é€è¯·æ±‚
            headers = get_random_headers()
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code != 200:
                print(f"  âŒ çŠ¶æ€ç : {response.status_code}")
                continue

            # 3. è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # 4. æå–æ•°æ®
            quotes = soup.find_all('div', class_='quote')

            print(f"  æ‰¾åˆ° {len(quotes)} æ¡åè¨€")

            for quote in quotes:
                # æå–åè¨€å†…å®¹
                text = quote.find('span', class_='text').text

                # æå–ä½œè€…
                author = quote.find('small', class_='author').text

                # æå–æ ‡ç­¾
                tags = quote.find_all('a', class_='tag')
                tag_list = [tag.text for tag in tags]

                # ä¿å­˜æ•°æ®
                quote_data = {
                    'text': text,
                    'author': author,
                    'tags': ', '.join(tag_list)
                }
                all_quotes.append(quote_data)

                # æ˜¾ç¤º
                print(f"  - {author}: {text[:30]}...")

            # 5. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            next_btn = soup.find('li', class_='next')
            if not next_btn:
                print("  å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
            continue

    # 6. ä¿å­˜æ•°æ®
    print(f"\nå…±çˆ¬å– {len(all_quotes)} æ¡åè¨€")

    if all_quotes:
        save_dir = "/mnt/c/dev/python/qqstudy/web_crawler/data"
        os.makedirs(save_dir, exist_ok=True)
        csv_file = os.path.join(save_dir, "quotes.csv")

        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['text', 'author', 'tags'])
            writer.writeheader()
            writer.writerows(all_quotes)

        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {csv_file}")

    return all_quotes


# ============================================================
# ç»ƒä¹ é¢˜
# ============================================================
"""
ğŸ“ ç»ƒä¹ é¢˜

è¯·å®Œæˆ /mnt/c/dev/python/qqstudy/web_crawler/web_crawler_exercises.py ä¸­çš„ç»ƒä¹ 

ç»ƒä¹ å†…å®¹åŒ…æ‹¬ï¼š
1. å‘é€HTTPè¯·æ±‚
2. è§£æHTML
3. æå–æ•°æ®
4. ä¿å­˜æ•°æ®
5. ç¿»é¡µçˆ¬å–
6. ç»¼åˆç»ƒä¹ 

æ¯é“é¢˜éƒ½æœ‰å‚è€ƒç­”æ¡ˆï¼Œå»ºè®®å…ˆè‡ªå·±å°è¯•ï¼Œå†çœ‹ç­”æ¡ˆã€‚
"""


# ============================================================
# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
# ============================================================
def run_all_sections():
    """
    è¿è¡Œæ‰€æœ‰ç« èŠ‚çš„ç¤ºä¾‹

    å‚æ•°è¯´æ˜ï¼š
        æ— å‚æ•°

    è°ƒç”¨ç¤ºä¾‹ï¼š
        run_all_sections()
    """
    print("\n" + "ğŸš€" * 30)
    print("å¼€å§‹è¿è¡Œæ‰€æœ‰ç« èŠ‚ç¤ºä¾‹...")
    print("ğŸš€" * 30)

    section_1_what_is_crawler()  # æ¦‚å¿µä»‹ç»
    section_2_http_basics()      # HTTPè¯·æ±‚
    section_3_parse_html()       # HTMLè§£æ
    section_4_extract_data()     # æ•°æ®æå–
    section_5_save_data()        # ä¿å­˜æ•°æ®
    section_6_anti_crawler()     # åçˆ¬è™«
    section_7_pagination()       # ç¿»é¡µçˆ¬å–
    # section_8_complete_example() # ç»¼åˆæ¡ˆä¾‹ï¼ˆéœ€è¦ç½‘ç»œï¼‰

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰ç« èŠ‚æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)

    print("""
ğŸ“š æ¥ä¸‹æ¥ä½ å¯ä»¥ï¼š

1. è¿è¡Œç»ƒä¹ é¢˜ï¼š
   python web_crawler_exercises.py

2. æŸ¥çœ‹ç¤ºä¾‹ä»£ç ï¼š
   - examples/crawl_quotes.py    # çˆ¬å–åè¨€
   - examples/crawl_weather.py   # çˆ¬å–å¤©æ°”
   - examples/crawl_books.py     # çˆ¬å–ä¹¦ç±

3. å¼€å§‹è‡ªå·±çš„çˆ¬è™«é¡¹ç›®ï¼

âš ï¸ è®°ä½ï¼šçˆ¬è™«è¦åˆæ³•åˆè§„ï¼
    """)


# ============================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================
if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘           ğŸ•·ï¸  ç½‘ç»œçˆ¬è™«é€Ÿæˆæ•™ç¨‹  ğŸ•·ï¸                      â•‘
â•‘                                                          â•‘
â•‘           é€‚åˆåˆä¸­ç”ŸåŠç¼–ç¨‹åˆå­¦è€…                          â•‘
â•‘           å­¦ä¹ æ—¶é—´ï¼šçº¦2-3å°æ—¶                             â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€ç›®å½•ã€‘
ç¬¬1èŠ‚ï¼šä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«ï¼Ÿï¼ˆæ¦‚å¿µã€åŸç†ã€æ³•å¾‹é“å¾·ï¼‰
ç¬¬2èŠ‚ï¼šHTTP è¯·æ±‚åŸºç¡€ï¼ˆrequestsåº“ã€GETè¯·æ±‚ã€çŠ¶æ€ç ï¼‰
ç¬¬3èŠ‚ï¼šè§£æ HTMLï¼ˆBeautifulSoupã€find/find_allï¼‰
ç¬¬4èŠ‚ï¼šæå–æ•°æ®ï¼ˆè·å–æ–‡æœ¬ã€å±æ€§ã€CSSé€‰æ‹©å™¨ï¼‰
ç¬¬5èŠ‚ï¼šä¿å­˜æ•°æ®ï¼ˆCSVã€JSONã€æ•°æ®åº“ï¼‰
ç¬¬6èŠ‚ï¼šå¤„ç†åçˆ¬è™«ï¼ˆUser-Agentã€è¯·æ±‚å¤´ã€å»¶è¿Ÿï¼‰
ç¬¬7èŠ‚ï¼šç¿»é¡µçˆ¬å–ï¼ˆåˆ†æåˆ†é¡µã€å¾ªç¯çˆ¬å–ï¼‰
ç¬¬8èŠ‚ï¼šç»¼åˆæ¡ˆä¾‹

ã€ä½¿ç”¨æ–¹æ³•ã€‘
1. è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ï¼š
   run_all_sections()

2. è¿è¡Œå•ä¸ªç« èŠ‚ï¼š
   section_1_what_is_crawler()  # ç¬¬1èŠ‚
   section_2_http_basics()      # ç¬¬2èŠ‚
   ...

3. è¿è¡Œç»¼åˆæ¡ˆä¾‹ï¼ˆéœ€è¦ç½‘ç»œï¼‰ï¼š
   section_8_complete_example()

ã€ä¾èµ–å®‰è£…ã€‘
pip install requests beautifulsoup4

âš ï¸ é‡è¦æé†’ï¼šçˆ¬è™«è¦åˆæ³•åˆè§„ï¼
- éµå®ˆç½‘ç«™çš„ robots.txt è§„åˆ™
- ä¸è¦é¢‘ç¹è¯·æ±‚ï¼Œç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
- ä¸è¦çˆ¬å–éšç§ä¿¡æ¯å’Œä»˜è´¹å†…å®¹
    """)

    # æç¤ºç”¨æˆ·è¾“å…¥
    print("\nè¯·é€‰æ‹©æ“ä½œï¼š")
    print("1. è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ï¼ˆä¸éœ€è¦ç½‘ç»œçš„éƒ¨åˆ†ï¼‰")
    print("2. åªè¿è¡Œç»¼åˆæ¡ˆä¾‹ï¼ˆéœ€è¦ç½‘ç»œï¼‰")
    print("3. é€€å‡º")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()

    if choice == '1':
        run_all_sections()
    elif choice == '2':
        section_8_complete_example()
    else:
        print("å†è§ï¼ç»§ç»­åŠ æ²¹å­¦ä¹ ï¼")
