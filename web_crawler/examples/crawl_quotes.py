# -*- coding: utf-8 -*-
"""
ç¤ºä¾‹1ï¼šçˆ¬å–åè¨€ç½‘ç«™
====================

ç›®æ ‡ç½‘ç«™ï¼šhttp://quotes.toscrape.com
è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå­¦ä¹ çˆ¬è™«çš„ç½‘ç«™

çˆ¬å–å†…å®¹ï¼š
- åè¨€å†…å®¹
- ä½œè€…
- æ ‡ç­¾

è¿è¡Œæ–¹æ³•ï¼š
python crawl_quotes.py
"""

import requests
from bs4 import BeautifulSoup
import csv
import json
import time
import random
import os


def get_headers():
    """
    è·å–éšæœºè¯·æ±‚å¤´

    è¿”å›å€¼ï¼š
        dict: è¯·æ±‚å¤´å­—å…¸
    """
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }


def crawl_quotes(max_pages=3):
    """
    çˆ¬å–åè¨€ç½‘ç«™

    å‚æ•°è¯´æ˜ï¼š
        max_pages (int): æœ€å¤§çˆ¬å–é¡µæ•°ï¼Œé»˜è®¤3é¡µ

    è¿”å›å€¼ï¼š
        list: åè¨€åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸

    è°ƒç”¨ç¤ºä¾‹ï¼š
        quotes = crawl_quotes(5)  # çˆ¬å–5é¡µ
        quotes = crawl_quotes()   # çˆ¬å–3é¡µï¼ˆé»˜è®¤ï¼‰
    """
    print("=" * 60)
    print("ğŸ•·ï¸ å¼€å§‹çˆ¬å–åè¨€ç½‘ç«™")
    print("=" * 60)
    print(f"ç›®æ ‡: http://quotes.toscrape.com")
    print(f"è®¡åˆ’çˆ¬å–: {max_pages} é¡µ")
    print("-" * 60)

    base_url = "http://quotes.toscrape.com/page/{}/"
    all_quotes = []

    for page in range(1, max_pages + 1):
        url = base_url.format(page)
        print(f"\nğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼ˆ1-2ç§’ï¼‰
            delay = random.uniform(1, 2)
            time.sleep(delay)

            # å‘é€è¯·æ±‚
            headers = get_headers()
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code != 200:
                print(f"  âŒ çŠ¶æ€ç : {response.status_code}")
                continue

            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–åè¨€
            quotes = soup.find_all('div', class_='quote')
            print(f"  æ‰¾åˆ° {len(quotes)} æ¡åè¨€")

            for quote in quotes:
                # æå–å†…å®¹
                text = quote.find('span', class_='text').text

                # æå–ä½œè€…
                author = quote.find('small', class_='author').text

                # æå–æ ‡ç­¾
                tags = quote.find_all('a', class_='tag')
                tag_list = [tag.text for tag in tags]

                # ä¿å­˜æ•°æ®
                quote_data = {
                    'text': text,
                    'author': author,
                    'tags': tag_list
                }
                all_quotes.append(quote_data)

                # æ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦ï¼‰
                display_text = text[:50] + "..." if len(text) > 50 else text
                print(f"  âœ… {author}: {display_text}")

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            next_btn = soup.find('li', class_='next')
            if not next_btn:
                print("  ğŸ“Œ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

        except requests.exceptions.ConnectionError:
            print("  âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
            break
        except requests.exceptions.Timeout:
            print("  âŒ è¯·æ±‚è¶…æ—¶")
            continue
        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
            continue

    print("\n" + "=" * 60)
    print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_quotes)} æ¡åè¨€")
    print("=" * 60)

    return all_quotes


def save_to_csv(data, filepath):
    """
    ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶

    å‚æ•°è¯´æ˜ï¼š
        data (list): æ•°æ®åˆ—è¡¨
        filepath (str): ä¿å­˜è·¯å¾„
    """
    if not data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return

    with open(filepath, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['author', 'text', 'tags'])

        writer.writeheader()

        for item in data:
            # å°†æ ‡ç­¾åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
            row = {
                'author': item['author'],
                'text': item['text'],
                'tags': ', '.join(item['tags'])
            }
            writer.writerow(row)

    print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {filepath}")


def save_to_json(data, filepath):
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶

    å‚æ•°è¯´æ˜ï¼š
        data (list): æ•°æ®åˆ—è¡¨
        filepath (str): ä¿å­˜è·¯å¾„
    """
    if not data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {filepath}")


def show_statistics(quotes):
    """
    æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯

    å‚æ•°è¯´æ˜ï¼š
        quotes (list): åè¨€åˆ—è¡¨
    """
    if not quotes:
        return

    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    print("-" * 40)

    # ç»Ÿè®¡ä½œè€…
    authors = [q['author'] for q in quotes]
    author_count = {}
    for author in authors:
        author_count[author] = author_count.get(author, 0) + 1

    # æ˜¾ç¤ºå‡ºç°æ¬¡æ•°æœ€å¤šçš„å‰5ä½ä½œè€…
    sorted_authors = sorted(author_count.items(), key=lambda x: x[1], reverse=True)
    print("åè¨€æœ€å¤šçš„ä½œè€…:")
    for author, count in sorted_authors[:5]:
        print(f"  - {author}: {count} æ¡")

    # ç»Ÿè®¡æ ‡ç­¾
    all_tags = []
    for q in quotes:
        all_tags.extend(q['tags'])

    tag_count = {}
    for tag in all_tags:
        tag_count[tag] = tag_count.get(tag, 0) + 1

    sorted_tags = sorted(tag_count.items(), key=lambda x: x[1], reverse=True)
    print("\næœ€å¸¸è§çš„æ ‡ç­¾:")
    for tag, count in sorted_tags[:5]:
        print(f"  - {tag}: {count} æ¬¡")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘           ğŸ“š åè¨€ç½‘ç«™çˆ¬è™«  ğŸ“š                            â•‘
â•‘                                                          â•‘
â•‘           ç›®æ ‡: http://quotes.toscrape.com               â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # è®¾ç½®ä¿å­˜ç›®å½•
    save_dir = "/mnt/c/dev/python/qqstudy/web_crawler/data"
    os.makedirs(save_dir, exist_ok=True)

    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        pages = input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°ï¼ˆç›´æ¥å›è½¦é»˜è®¤3é¡µï¼‰: ").strip()
        max_pages = int(pages) if pages else 3
    except ValueError:
        max_pages = 3

    # çˆ¬å–æ•°æ®
    quotes = crawl_quotes(max_pages)

    if quotes:
        # ä¿å­˜æ•°æ®
        csv_file = os.path.join(save_dir, "quotes.csv")
        json_file = os.path.join(save_dir, "quotes.json")

        save_to_csv(quotes, csv_file)
        save_to_json(quotes, json_file)

        # æ˜¾ç¤ºç»Ÿè®¡
        show_statistics(quotes)

        # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
        print("\nğŸ“– ç¤ºä¾‹æ•°æ®ï¼ˆå‰3æ¡ï¼‰:")
        print("-" * 40)
        for i, quote in enumerate(quotes[:3], 1):
            print(f"\n{i}. {quote['author']}:")
            print(f"   \"{quote['text']}\"")
            print(f"   æ ‡ç­¾: {', '.join(quote['tags'])}")

    print("\n" + "=" * 60)
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
